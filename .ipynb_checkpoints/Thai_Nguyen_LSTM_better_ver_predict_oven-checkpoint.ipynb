{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LcAqTEOMjYMn"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install pandas_datareader\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0HH8gQW8jYMv"
   },
   "outputs": [],
   "source": [
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_rows', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RCjOj0oNjYMy"
   },
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('https://raw.githubusercontent.com/minhthai1995/FIT5149/main/train_data_withlabels.csv')\n",
    "dataset_test = pd.read_csv('https://raw.githubusercontent.com/minhthai1995/FIT5149/main/test_data_nolabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DXlKjIpRjYMz",
    "outputId": "7e0bf74e-7e1e-4a56-b78f-9361b93820e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>load</th>\n",
       "      <th>ac</th>\n",
       "      <th>ev</th>\n",
       "      <th>oven</th>\n",
       "      <th>wash</th>\n",
       "      <th>dryer</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dif</th>\n",
       "      <th>absdif</th>\n",
       "      <th>max</th>\n",
       "      <th>var</th>\n",
       "      <th>entropy</th>\n",
       "      <th>nonlinear</th>\n",
       "      <th>hurst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>417715</th>\n",
       "      <td>523256</td>\n",
       "      <td>2.543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417716</th>\n",
       "      <td>523257</td>\n",
       "      <td>2.417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417717</th>\n",
       "      <td>523258</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>1.418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417718</th>\n",
       "      <td>523259</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417719</th>\n",
       "      <td>523260</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   load  ac  ev  oven  wash  dryer  hourofday dayofweek  \\\n",
       "417715      523256  2.543   0   0     0     0      0         21       Tue   \n",
       "417716      523257  2.417   0   0     0     0      0         21       Tue   \n",
       "417717      523258  0.999   0   0     0     0      0         21       Tue   \n",
       "417718      523259  0.966   0   0     0     0      0         21       Tue   \n",
       "417719      523260  0.964   0   0     0     0      0         21       Tue   \n",
       "\n",
       "          dif  absdif  max  var  entropy  nonlinear  hurst  \n",
       "417715 -0.003   0.003  0.0  0.0      0.0        0.0    0.0  \n",
       "417716 -0.126   0.126  0.0  0.0      0.0        0.0    0.0  \n",
       "417717 -1.418   1.418  0.0  0.0      0.0        0.0    0.0  \n",
       "417718 -0.033   0.033  0.0  0.0      0.0        0.0    0.0  \n",
       "417719 -0.002   0.002  0.0  0.0      0.0        0.0    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Vmso9W__jYNC",
    "outputId": "34cd1a83-0b7a-41bb-fb16-ae968ccd938d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>load</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dif</th>\n",
       "      <th>absdif</th>\n",
       "      <th>max</th>\n",
       "      <th>var</th>\n",
       "      <th>entropy</th>\n",
       "      <th>nonlinear</th>\n",
       "      <th>hurst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.869</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.673</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.660</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.772</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.679</td>\n",
       "      <td>0</td>\n",
       "      <td>Mon</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   load  hourofday dayofweek    dif  absdif  max  var  entropy  \\\n",
       "0           1  1.869          0       Mon  0.000   0.000  0.0  0.0      0.0   \n",
       "1           2  1.673          0       Mon -0.196   0.196  0.0  0.0      0.0   \n",
       "2           3  1.660          0       Mon -0.013   0.013  0.0  0.0      0.0   \n",
       "3           4  1.772          0       Mon  0.112   0.112  0.0  0.0      0.0   \n",
       "4           5  1.679          0       Mon -0.093   0.093  0.0  0.0      0.0   \n",
       "\n",
       "   nonlinear  hurst  \n",
       "0        0.0    0.0  \n",
       "1        0.0    0.0  \n",
       "2        0.0    0.0  \n",
       "3        0.0    0.0  \n",
       "4        0.0    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "le_iXBFKjYNF"
   },
   "outputs": [],
   "source": [
    "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIkExfOGjYNR",
    "outputId": "80599878-43de-4b93-b956-9cf39c5ee542"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((417720, 16), (105540, 11), (523260, 16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape, dataset_test.shape, dataset_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Qyw513xhjYNT",
    "outputId": "960fc6bb-1583-4653-eb74-b0cbc673cb68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>load</th>\n",
       "      <th>ac</th>\n",
       "      <th>ev</th>\n",
       "      <th>oven</th>\n",
       "      <th>wash</th>\n",
       "      <th>dryer</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dif</th>\n",
       "      <th>absdif</th>\n",
       "      <th>max</th>\n",
       "      <th>var</th>\n",
       "      <th>entropy</th>\n",
       "      <th>nonlinear</th>\n",
       "      <th>hurst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105541</td>\n",
       "      <td>2.245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.987</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.074549</td>\n",
       "      <td>0.678886</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>0.994071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105542</td>\n",
       "      <td>2.259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.172867</td>\n",
       "      <td>0.667450</td>\n",
       "      <td>0.054829</td>\n",
       "      <td>0.994154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105543</td>\n",
       "      <td>2.269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.270112</td>\n",
       "      <td>0.647777</td>\n",
       "      <td>0.056991</td>\n",
       "      <td>0.994220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105544</td>\n",
       "      <td>2.268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.303763</td>\n",
       "      <td>0.629227</td>\n",
       "      <td>0.057606</td>\n",
       "      <td>0.994150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105545</td>\n",
       "      <td>2.270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.302744</td>\n",
       "      <td>0.621295</td>\n",
       "      <td>0.082640</td>\n",
       "      <td>0.994041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523255</th>\n",
       "      <td>105536</td>\n",
       "      <td>1.076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>Sat</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6.204</td>\n",
       "      <td>2.484129</td>\n",
       "      <td>0.592658</td>\n",
       "      <td>0.042413</td>\n",
       "      <td>0.990185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523256</th>\n",
       "      <td>105537</td>\n",
       "      <td>1.019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>Sat</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.057</td>\n",
       "      <td>6.215</td>\n",
       "      <td>2.614626</td>\n",
       "      <td>0.601761</td>\n",
       "      <td>0.038118</td>\n",
       "      <td>0.990529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523257</th>\n",
       "      <td>105538</td>\n",
       "      <td>1.014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>Sat</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6.215</td>\n",
       "      <td>2.729923</td>\n",
       "      <td>0.629122</td>\n",
       "      <td>0.048229</td>\n",
       "      <td>0.993554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523258</th>\n",
       "      <td>105539</td>\n",
       "      <td>1.011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>Sat</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>6.215</td>\n",
       "      <td>2.868153</td>\n",
       "      <td>0.658328</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.993813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523259</th>\n",
       "      <td>105540</td>\n",
       "      <td>1.258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>Sat</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.247</td>\n",
       "      <td>6.215</td>\n",
       "      <td>2.972294</td>\n",
       "      <td>0.676443</td>\n",
       "      <td>0.049666</td>\n",
       "      <td>0.993965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523260 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   load   ac   ev  oven  wash  dryer  hourofday dayofweek  \\\n",
       "0           105541  2.245  0.0  0.0   0.0   0.0    0.0          0       Sun   \n",
       "1           105542  2.259  0.0  0.0   0.0   0.0    0.0          0       Sun   \n",
       "2           105543  2.269  0.0  0.0   0.0   0.0    0.0          0       Sun   \n",
       "3           105544  2.268  0.0  0.0   0.0   0.0    0.0          0       Sun   \n",
       "4           105545  2.270  0.0  0.0   0.0   0.0    0.0          0       Sun   \n",
       "...            ...    ...  ...  ...   ...   ...    ...        ...       ...   \n",
       "523255      105536  1.076  NaN  NaN   NaN   NaN    NaN         23       Sat   \n",
       "523256      105537  1.019  NaN  NaN   NaN   NaN    NaN         23       Sat   \n",
       "523257      105538  1.014  NaN  NaN   NaN   NaN    NaN         23       Sat   \n",
       "523258      105539  1.011  NaN  NaN   NaN   NaN    NaN         23       Sat   \n",
       "523259      105540  1.258  NaN  NaN   NaN   NaN    NaN         23       Sat   \n",
       "\n",
       "          dif  absdif    max       var   entropy  nonlinear     hurst  \n",
       "0       0.987   0.987  6.215  3.074549  0.678886   0.052903  0.994071  \n",
       "1       0.014   0.014  6.215  3.172867  0.667450   0.054829  0.994154  \n",
       "2       0.010   0.010  6.215  3.270112  0.647777   0.056991  0.994220  \n",
       "3      -0.001   0.001  6.215  3.303763  0.629227   0.057606  0.994150  \n",
       "4       0.002   0.002  6.215  3.302744  0.621295   0.082640  0.994041  \n",
       "...       ...     ...    ...       ...       ...        ...       ...  \n",
       "523255 -0.001   0.001  6.204  2.484129  0.592658   0.042413  0.990185  \n",
       "523256 -0.057   0.057  6.215  2.614626  0.601761   0.038118  0.990529  \n",
       "523257 -0.005   0.005  6.215  2.729923  0.629122   0.048229  0.993554  \n",
       "523258 -0.003   0.003  6.215  2.868153  0.658328   0.046729  0.993813  \n",
       "523259  0.247   0.247  6.215  2.972294  0.676443   0.049666  0.993965  \n",
       "\n",
       "[523260 rows x 16 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total.reset_index(inplace=True, drop=True) \n",
    "dataset_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0f7bEuspjYNV"
   },
   "outputs": [],
   "source": [
    "scale_list = ['absdif', 'dif', 'entropy', 'hurst', 'load', 'max', 'nonlinear', 'var']\n",
    "scaler = preprocessing.MinMaxScaler().fit(dataset_total[scale_list])\n",
    "dataset_total[scale_list] = scaler.transform(dataset_total[scale_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XYjX3f4ljYNa"
   },
   "outputs": [],
   "source": [
    "labelEncoderDay = preprocessing.LabelEncoder().fit(dataset_total['dayofweek'])\n",
    "labelEncoderHour = preprocessing.LabelEncoder().fit(dataset_total['hourofday'])\n",
    "dataset_total['dayofweek'] = labelEncoderDay.transform(dataset_total['dayofweek'])\n",
    "dataset_total['hourofday'] = labelEncoderHour.transform(dataset_total['hourofday'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0FptlHXIxFrV"
   },
   "outputs": [],
   "source": [
    "label_list = ['ac', 'ev', 'oven', 'wash', 'dryer']\n",
    "dataset_total[label_list] = dataset_total[label_list].fillna(0.0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "1Cj3QPEbjYNd",
    "outputId": "0d3fae26-4950-4e18-d346-c81a386cd0a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>load</th>\n",
       "      <th>ac</th>\n",
       "      <th>ev</th>\n",
       "      <th>oven</th>\n",
       "      <th>wash</th>\n",
       "      <th>dryer</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dif</th>\n",
       "      <th>absdif</th>\n",
       "      <th>max</th>\n",
       "      <th>var</th>\n",
       "      <th>entropy</th>\n",
       "      <th>nonlinear</th>\n",
       "      <th>hurst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523255</th>\n",
       "      <td>105536</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511194</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.509611</td>\n",
       "      <td>0.151982</td>\n",
       "      <td>0.592666</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.993362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523256</th>\n",
       "      <td>105537</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.507602</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.159966</td>\n",
       "      <td>0.601769</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.993707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523257</th>\n",
       "      <td>105538</td>\n",
       "      <td>0.060290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.510937</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.167020</td>\n",
       "      <td>0.629130</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.996741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523258</th>\n",
       "      <td>105539</td>\n",
       "      <td>0.060037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511065</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.175477</td>\n",
       "      <td>0.658337</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.997001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523259</th>\n",
       "      <td>105540</td>\n",
       "      <td>0.080835</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.527102</td>\n",
       "      <td>0.030991</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.181849</td>\n",
       "      <td>0.676451</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.997154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      load  ac  ev  oven  wash  dryer  hourofday  dayofweek  \\\n",
       "523255      105536  0.065510   0   0     0     0      0         23          2   \n",
       "523256      105537  0.060711   0   0     0     0      0         23          2   \n",
       "523257      105538  0.060290   0   0     0     0      0         23          2   \n",
       "523258      105539  0.060037   0   0     0     0      0         23          2   \n",
       "523259      105540  0.080835   0   0     0     0      0         23          2   \n",
       "\n",
       "             dif    absdif       max       var   entropy  nonlinear     hurst  \n",
       "523255  0.511194  0.000125  0.509611  0.151982  0.592666   0.000647  0.993362  \n",
       "523256  0.507602  0.007152  0.510514  0.159966  0.601769   0.000581  0.993707  \n",
       "523257  0.510937  0.000627  0.510514  0.167020  0.629130   0.000735  0.996741  \n",
       "523258  0.511065  0.000376  0.510514  0.175477  0.658337   0.000712  0.997001  \n",
       "523259  0.527102  0.030991  0.510514  0.181849  0.676451   0.000757  0.997154  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "A00sup4kjYNf"
   },
   "outputs": [],
   "source": [
    "features_set = dataset_total.loc[:,dataset_total.columns.difference(['ac', 'ev', 'oven', 'wash', 'dryer' ])]\n",
    "features_set.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "# labels_set = dataset_total[['ac', 'ev', 'oven', 'wash', 'dryer']]\n",
    "labels_set = dataset_total[['oven']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "lksBdOQgjYNk",
    "outputId": "65f74508-8991-4476-921a-1be2e786ee34"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absdif</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dif</th>\n",
       "      <th>entropy</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>hurst</th>\n",
       "      <th>load</th>\n",
       "      <th>max</th>\n",
       "      <th>nonlinear</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123839</td>\n",
       "      <td>3</td>\n",
       "      <td>0.574572</td>\n",
       "      <td>0.678894</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.163944</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.188105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001757</td>\n",
       "      <td>3</td>\n",
       "      <td>0.512156</td>\n",
       "      <td>0.667459</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997343</td>\n",
       "      <td>0.165123</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.194120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001255</td>\n",
       "      <td>3</td>\n",
       "      <td>0.511899</td>\n",
       "      <td>0.647785</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997409</td>\n",
       "      <td>0.165965</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.200070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>3</td>\n",
       "      <td>0.511194</td>\n",
       "      <td>0.629235</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997339</td>\n",
       "      <td>0.165881</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.202129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000251</td>\n",
       "      <td>3</td>\n",
       "      <td>0.511386</td>\n",
       "      <td>0.621303</td>\n",
       "      <td>0</td>\n",
       "      <td>0.997229</td>\n",
       "      <td>0.166049</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.202066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523255</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511194</td>\n",
       "      <td>0.592666</td>\n",
       "      <td>23</td>\n",
       "      <td>0.993362</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0.509611</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.151982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523256</th>\n",
       "      <td>0.007152</td>\n",
       "      <td>2</td>\n",
       "      <td>0.507602</td>\n",
       "      <td>0.601769</td>\n",
       "      <td>23</td>\n",
       "      <td>0.993707</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.159966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523257</th>\n",
       "      <td>0.000627</td>\n",
       "      <td>2</td>\n",
       "      <td>0.510937</td>\n",
       "      <td>0.629130</td>\n",
       "      <td>23</td>\n",
       "      <td>0.996741</td>\n",
       "      <td>0.060290</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.167020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523258</th>\n",
       "      <td>0.000376</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511065</td>\n",
       "      <td>0.658337</td>\n",
       "      <td>23</td>\n",
       "      <td>0.997001</td>\n",
       "      <td>0.060037</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.175477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523259</th>\n",
       "      <td>0.030991</td>\n",
       "      <td>2</td>\n",
       "      <td>0.527102</td>\n",
       "      <td>0.676451</td>\n",
       "      <td>23</td>\n",
       "      <td>0.997154</td>\n",
       "      <td>0.080835</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.181849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523260 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          absdif  dayofweek       dif   entropy  hourofday     hurst  \\\n",
       "0       0.123839          3  0.574572  0.678894          0  0.997260   \n",
       "1       0.001757          3  0.512156  0.667459          0  0.997343   \n",
       "2       0.001255          3  0.511899  0.647785          0  0.997409   \n",
       "3       0.000125          3  0.511194  0.629235          0  0.997339   \n",
       "4       0.000251          3  0.511386  0.621303          0  0.997229   \n",
       "...          ...        ...       ...       ...        ...       ...   \n",
       "523255  0.000125          2  0.511194  0.592666         23  0.993362   \n",
       "523256  0.007152          2  0.507602  0.601769         23  0.993707   \n",
       "523257  0.000627          2  0.510937  0.629130         23  0.996741   \n",
       "523258  0.000376          2  0.511065  0.658337         23  0.997001   \n",
       "523259  0.030991          2  0.527102  0.676451         23  0.997154   \n",
       "\n",
       "            load       max  nonlinear       var  \n",
       "0       0.163944  0.510514   0.000807  0.188105  \n",
       "1       0.165123  0.510514   0.000836  0.194120  \n",
       "2       0.165965  0.510514   0.000869  0.200070  \n",
       "3       0.165881  0.510514   0.000878  0.202129  \n",
       "4       0.166049  0.510514   0.001260  0.202066  \n",
       "...          ...       ...        ...       ...  \n",
       "523255  0.065510  0.509611   0.000647  0.151982  \n",
       "523256  0.060711  0.510514   0.000581  0.159966  \n",
       "523257  0.060290  0.510514   0.000735  0.167020  \n",
       "523258  0.060037  0.510514   0.000712  0.175477  \n",
       "523259  0.080835  0.510514   0.000757  0.181849  \n",
       "\n",
       "[523260 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9M5iBLCq36ns"
   },
   "outputs": [],
   "source": [
    "#Take knowledge from the back 10 mins window, 1 mins each\n",
    "for x in np.arange(1, 10, 1):\n",
    "    features_set['load_before_' + str(x)] = features_set['load'].shift(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "C2_nMfYc36wU"
   },
   "outputs": [],
   "source": [
    "features_set = features_set.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "xOwLlkvA36zO",
    "outputId": "30fb1217-9b1b-4f54-fc00-644ef469cbc1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absdif</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dif</th>\n",
       "      <th>entropy</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>hurst</th>\n",
       "      <th>load</th>\n",
       "      <th>max</th>\n",
       "      <th>nonlinear</th>\n",
       "      <th>var</th>\n",
       "      <th>load_before_1</th>\n",
       "      <th>load_before_2</th>\n",
       "      <th>load_before_3</th>\n",
       "      <th>load_before_4</th>\n",
       "      <th>load_before_5</th>\n",
       "      <th>load_before_6</th>\n",
       "      <th>load_before_7</th>\n",
       "      <th>load_before_8</th>\n",
       "      <th>load_before_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001631</td>\n",
       "      <td>3</td>\n",
       "      <td>0.512092</td>\n",
       "      <td>0.650014</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995492</td>\n",
       "      <td>0.166639</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.195543</td>\n",
       "      <td>0.165544</td>\n",
       "      <td>0.164449</td>\n",
       "      <td>0.164365</td>\n",
       "      <td>0.165123</td>\n",
       "      <td>0.166049</td>\n",
       "      <td>0.165881</td>\n",
       "      <td>0.165965</td>\n",
       "      <td>0.165123</td>\n",
       "      <td>0.163944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.003764</td>\n",
       "      <td>3</td>\n",
       "      <td>0.513182</td>\n",
       "      <td>0.638492</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995509</td>\n",
       "      <td>0.169165</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>0.195310</td>\n",
       "      <td>0.166639</td>\n",
       "      <td>0.165544</td>\n",
       "      <td>0.164449</td>\n",
       "      <td>0.164365</td>\n",
       "      <td>0.165123</td>\n",
       "      <td>0.166049</td>\n",
       "      <td>0.165881</td>\n",
       "      <td>0.165965</td>\n",
       "      <td>0.165123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.009661</td>\n",
       "      <td>3</td>\n",
       "      <td>0.516197</td>\n",
       "      <td>0.627796</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995471</td>\n",
       "      <td>0.175648</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.193990</td>\n",
       "      <td>0.169165</td>\n",
       "      <td>0.166639</td>\n",
       "      <td>0.165544</td>\n",
       "      <td>0.164449</td>\n",
       "      <td>0.164365</td>\n",
       "      <td>0.165123</td>\n",
       "      <td>0.166049</td>\n",
       "      <td>0.165881</td>\n",
       "      <td>0.165965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000251</td>\n",
       "      <td>3</td>\n",
       "      <td>0.511130</td>\n",
       "      <td>0.625368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995359</td>\n",
       "      <td>0.175480</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.012250</td>\n",
       "      <td>0.191879</td>\n",
       "      <td>0.175648</td>\n",
       "      <td>0.169165</td>\n",
       "      <td>0.166639</td>\n",
       "      <td>0.165544</td>\n",
       "      <td>0.164449</td>\n",
       "      <td>0.164365</td>\n",
       "      <td>0.165123</td>\n",
       "      <td>0.166049</td>\n",
       "      <td>0.165881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007403</td>\n",
       "      <td>3</td>\n",
       "      <td>0.515043</td>\n",
       "      <td>0.633312</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995880</td>\n",
       "      <td>0.180448</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>0.189279</td>\n",
       "      <td>0.175480</td>\n",
       "      <td>0.175648</td>\n",
       "      <td>0.169165</td>\n",
       "      <td>0.166639</td>\n",
       "      <td>0.165544</td>\n",
       "      <td>0.164449</td>\n",
       "      <td>0.164365</td>\n",
       "      <td>0.165123</td>\n",
       "      <td>0.166049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523255</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511194</td>\n",
       "      <td>0.592666</td>\n",
       "      <td>23</td>\n",
       "      <td>0.993362</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0.509611</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.151982</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>0.065005</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.064921</td>\n",
       "      <td>0.068542</td>\n",
       "      <td>0.071489</td>\n",
       "      <td>0.068542</td>\n",
       "      <td>0.072331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523256</th>\n",
       "      <td>0.007152</td>\n",
       "      <td>2</td>\n",
       "      <td>0.507602</td>\n",
       "      <td>0.601769</td>\n",
       "      <td>23</td>\n",
       "      <td>0.993707</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.159966</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>0.065005</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.064921</td>\n",
       "      <td>0.068542</td>\n",
       "      <td>0.071489</td>\n",
       "      <td>0.068542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523257</th>\n",
       "      <td>0.000627</td>\n",
       "      <td>2</td>\n",
       "      <td>0.510937</td>\n",
       "      <td>0.629130</td>\n",
       "      <td>23</td>\n",
       "      <td>0.996741</td>\n",
       "      <td>0.060290</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.167020</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>0.065005</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.064921</td>\n",
       "      <td>0.068542</td>\n",
       "      <td>0.071489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523258</th>\n",
       "      <td>0.000376</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511065</td>\n",
       "      <td>0.658337</td>\n",
       "      <td>23</td>\n",
       "      <td>0.997001</td>\n",
       "      <td>0.060037</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.175477</td>\n",
       "      <td>0.060290</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>0.065005</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.064921</td>\n",
       "      <td>0.068542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523259</th>\n",
       "      <td>0.030991</td>\n",
       "      <td>2</td>\n",
       "      <td>0.527102</td>\n",
       "      <td>0.676451</td>\n",
       "      <td>23</td>\n",
       "      <td>0.997154</td>\n",
       "      <td>0.080835</td>\n",
       "      <td>0.510514</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.181849</td>\n",
       "      <td>0.060037</td>\n",
       "      <td>0.060290</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>0.065005</td>\n",
       "      <td>0.065594</td>\n",
       "      <td>0.064921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523251 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          absdif  dayofweek       dif   entropy  hourofday     hurst  \\\n",
       "9       0.001631          3  0.512092  0.650014          0  0.995492   \n",
       "10      0.003764          3  0.513182  0.638492          0  0.995509   \n",
       "11      0.009661          3  0.516197  0.627796          0  0.995471   \n",
       "12      0.000251          3  0.511130  0.625368          0  0.995359   \n",
       "13      0.007403          3  0.515043  0.633312          0  0.995880   \n",
       "...          ...        ...       ...       ...        ...       ...   \n",
       "523255  0.000125          2  0.511194  0.592666         23  0.993362   \n",
       "523256  0.007152          2  0.507602  0.601769         23  0.993707   \n",
       "523257  0.000627          2  0.510937  0.629130         23  0.996741   \n",
       "523258  0.000376          2  0.511065  0.658337         23  0.997001   \n",
       "523259  0.030991          2  0.527102  0.676451         23  0.997154   \n",
       "\n",
       "            load       max  nonlinear       var  load_before_1  load_before_2  \\\n",
       "9       0.166639  0.510514   0.005589  0.195543       0.165544       0.164449   \n",
       "10      0.169165  0.510514   0.007342  0.195310       0.166639       0.165544   \n",
       "11      0.175648  0.510514   0.009588  0.193990       0.169165       0.166639   \n",
       "12      0.175480  0.510514   0.012250  0.191879       0.175648       0.169165   \n",
       "13      0.180448  0.510514   0.006325  0.189279       0.175480       0.175648   \n",
       "...          ...       ...        ...       ...            ...            ...   \n",
       "523255  0.065510  0.509611   0.000647  0.151982       0.065594       0.065173   \n",
       "523256  0.060711  0.510514   0.000581  0.159966       0.065510       0.065594   \n",
       "523257  0.060290  0.510514   0.000735  0.167020       0.060711       0.065510   \n",
       "523258  0.060037  0.510514   0.000712  0.175477       0.060290       0.060711   \n",
       "523259  0.080835  0.510514   0.000757  0.181849       0.060037       0.060290   \n",
       "\n",
       "        load_before_3  load_before_4  load_before_5  load_before_6  \\\n",
       "9            0.164365       0.165123       0.166049       0.165881   \n",
       "10           0.164449       0.164365       0.165123       0.166049   \n",
       "11           0.165544       0.164449       0.164365       0.165123   \n",
       "12           0.166639       0.165544       0.164449       0.164365   \n",
       "13           0.169165       0.166639       0.165544       0.164449   \n",
       "...               ...            ...            ...            ...   \n",
       "523255       0.065005       0.065594       0.064921       0.068542   \n",
       "523256       0.065173       0.065005       0.065594       0.064921   \n",
       "523257       0.065594       0.065173       0.065005       0.065594   \n",
       "523258       0.065510       0.065594       0.065173       0.065005   \n",
       "523259       0.060711       0.065510       0.065594       0.065173   \n",
       "\n",
       "        load_before_7  load_before_8  load_before_9  \n",
       "9            0.165965       0.165123       0.163944  \n",
       "10           0.165881       0.165965       0.165123  \n",
       "11           0.166049       0.165881       0.165965  \n",
       "12           0.165123       0.166049       0.165881  \n",
       "13           0.164365       0.165123       0.166049  \n",
       "...               ...            ...            ...  \n",
       "523255       0.071489       0.068542       0.072331  \n",
       "523256       0.068542       0.071489       0.068542  \n",
       "523257       0.064921       0.068542       0.071489  \n",
       "523258       0.065594       0.064921       0.068542  \n",
       "523259       0.065005       0.065594       0.064921  \n",
       "\n",
       "[523251 rows x 19 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ch_ZJlq4362o"
   },
   "outputs": [],
   "source": [
    "labels_set = labels_set.loc[9:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TJbpOzmF364p"
   },
   "outputs": [],
   "source": [
    "labels_set.reset_index(inplace=True, drop=True) \n",
    "features_set.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JQuIml035jlf"
   },
   "outputs": [],
   "source": [
    "X_train = features_set.loc[0:(dataset_train.shape[0] - 10),]\n",
    "y_train = labels_set.loc[0:(dataset_train.shape[0] - 10), ]\n",
    "\n",
    "#X_valid = labels_set \n",
    "#y_valid = \n",
    "X_test = features_set.loc[(dataset_train.shape[0] - 10):,]\n",
    "y_test = [] \n",
    "\n",
    "# I want to use a T-days window of input data for predicting target_class\n",
    "# It means I need to prepend (T-1) last train records to the 1st test window\n",
    "T = 30  # my choice of the timesteps window\n",
    "\n",
    "prepend_features = X_train.iloc[-(T-1):]\n",
    "X_test = pd.concat([prepend_features, X_test], axis=0)\n",
    "\n",
    "# Split the training into train and valid set\n",
    "X_train_final, X_valid, y_train_final, y_valid = train_test_split(X_train, y_train, test_size = 0.25, random_state = 42, shuffle=False)\n",
    "\n",
    "prepend_features_valid = X_train_final.loc[-(T-1):]\n",
    "X_valid = pd.concat([prepend_features_valid, X_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qUGcrFS5joa",
    "outputId": "014f9a19-f319-49ab-871d-18e346c39757"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313283, 19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dkeKqEmLjYNp"
   },
   "outputs": [],
   "source": [
    "# X_train = features_set.iloc[0:dataset_train.shape[0],]\n",
    "# y_train = labels_set.iloc[0:dataset_train.shape[0], ]\n",
    "\n",
    "# #X_valid = labels_set \n",
    "# #y_valid = \n",
    "# X_test = features_set.iloc[dataset_train.shape[0]:,]\n",
    "# y_test = [] \n",
    "\n",
    "# # I want to use a T-days window of input data for predicting target_class\n",
    "# # It means I need to prepend (T-1) last train records to the 1st test window\n",
    "# T = 30  # my choice of the timesteps window\n",
    "\n",
    "# prepend_features = X_train.iloc[-(T-1):]\n",
    "# X_test = pd.concat([prepend_features, X_test], axis=0)\n",
    "\n",
    "# # Split the training into train and valid set\n",
    "# X_train_final, X_valid, y_train_final, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42,shuffle=False)\n",
    "\n",
    "# prepend_features_valid = X_train_final.iloc[-(T-1):]\n",
    "# X_valid = pd.concat([prepend_features_valid, X_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IZmCj0SQVLc",
    "outputId": "72783643-fbff-4e0b-e702-c36118c9f9cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313283, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIOiZ55mQVjv",
    "outputId": "0ae37bd6-0f03-4e4a-f382-87d8c184f5df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313283, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Kf7T8uX0jYOD"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "for i in range(X_train_final.shape[0] - (T-1)):\n",
    "    X_train.append(X_train_final.iloc[i:i+T].values)\n",
    "    y_train.append(y_train_final.iloc[i + (T-1)].values)\n",
    "# X_train, y_train = np.array(X_train), np.array(y_train).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LsrcjQ17jYOH"
   },
   "outputs": [],
   "source": [
    "X_train_LSTM, y_train_LSTM = [] , []\n",
    "X_train_LSTM = np.array(X_train)\n",
    "y_train_LSTM = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QcC_kgXjYOI",
    "outputId": "b5e3d166-7118-4d31-9760-a4854f4f3300"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313254, 30, 19)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_LSTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaVg7_wEjYOh",
    "outputId": "d9c6ab62-9219-420a-ce79-f0d787973dc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313254, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_LSTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fhnaCRjjjYOi"
   },
   "outputs": [],
   "source": [
    "X_valid_LSTM, y_valid_LSTM = [], []\n",
    "for i in range(y_valid.shape[0]):\n",
    "    X_valid_LSTM.append(X_valid.iloc[i:i+T].values)\n",
    "    y_valid_LSTM.append(y_valid.iloc[i])\n",
    "X_valid_LSTM, y_valid_LSTM = np.array(X_valid_LSTM), np.array(y_valid_LSTM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKO8JtX2jYOj",
    "outputId": "f147f345-fec8-4888-84a9-35c4f8c8e3e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104428, 30, 19)\n",
      "(104428, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_valid_LSTM.shape)\n",
    "print(y_valid_LSTM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JNAonp3VjYOk"
   },
   "outputs": [],
   "source": [
    "X_test_LSTM = []\n",
    "for i in range(X_test.shape[0] - (T-1)):\n",
    "    X_test_LSTM.append(X_test.iloc[i:i+T].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fsy0H6ZUjYOl",
    "outputId": "24c3d638-1f73-4564-e07e-fa207125edec",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105541"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HPYwX-1tjYOm"
   },
   "outputs": [],
   "source": [
    "X_test_LSTM = np.array(X_test_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_x9_brsqjYOn",
    "outputId": "0ff03ea2-9ab0-4ee8-879a-4cb74946aa58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105541, 30, 19)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_LSTM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ske0A4hijYOn"
   },
   "source": [
    "LSTM Model - Batch Training and Predictiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "JTr_fBlijYOo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "wMiuKfzIjYOp"
   },
   "outputs": [],
   "source": [
    "# Import Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xp69XP3EjYOq",
    "outputId": "9400bb43-e3cc-4acd-e9a9-67862b573f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers=[8, 8, 8, 1], train_examples=313254, test_examples=105541\n",
      "batch = 313254, timesteps = 30, features = 19, epochs = 200\n",
      "lr = 0.05, lambda = 0.03, dropout = 0.0, recurr_dropout = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's make a list of CONSTANTS for modelling:\n",
    "LAYERS = [8, 8, 8, 1]                # number of units in hidden and output layers\n",
    "M_TRAIN = X_train_LSTM.shape[0]           # number of training examples (2D)\n",
    "M_TEST = X_test_LSTM.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = X_train_LSTM.shape[2]                 # number of features\n",
    "BATCH = M_TRAIN                          # batch size\n",
    "EPOCH = 200                           # number of epochs\n",
    "LR = 5e-2                            # learning rate of the gradient descent\n",
    "LAMBD = 3e-2                         # lambda in L2 regularizaion\n",
    "DP = 0.0                             # dropout rate\n",
    "RDP = 0.0                            # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "V6-I8kgl8Fg0"
   },
   "outputs": [],
   "source": [
    "y_1d = y_train_LSTM.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orn9oL1C00cB",
    "outputId": "931f963c-3fe9-4c07-ff77-515860739139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4771254774366954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.73997577])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = np.sum(y_1d == 1)\n",
    "neg = np.sum(y_1d == 0)\n",
    "print(pos/neg)\n",
    "total = pos + neg\n",
    "\n",
    "\n",
    "initial_bias = np.log([pos/neg])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "27ExT_Se1Exd"
   },
   "outputs": [],
   "source": [
    "output_bias = tf.keras.initializers.Constant(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "OxgW2-wPjYOs"
   },
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(T, N), units=LAYERS[0], return_sequences=True\n",
    "              #  activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "              #  kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              #  dropout=DP, recurrent_dropout=RDP,\n",
    "              #  return_sequences=True, return_state=False,\n",
    "              #  stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=LAYERS[1], return_sequences=True\n",
    "              #  activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "              #  kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              #  dropout=DP, recurrent_dropout=RDP,\n",
    "              #  return_sequences=True, return_state=False,\n",
    "              #  stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=LAYERS[2],\n",
    "              #  activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "              #  kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              #  dropout=DP, recurrent_dropout=RDP,\n",
    "              #  return_sequences=False, return_state=False,\n",
    "              #  stateful=False, unroll=False\n",
    "              ))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Dense(units=LAYERS[3], activation='sigmoid', bias_initializer=output_bias))\n",
    "\n",
    "model.add(Dense(units=LAYERS[3], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZydxoiwYEM0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkqeE7KV0gmI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEcUVjIDXl_9",
    "outputId": "bb4369ec-8f44-4681-fad0-70c4dddc64d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 30, 8)             896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 8)             32        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 8)             544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 8)             32        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 8)                 544       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,057\n",
      "Trainable params: 2,025\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy', f1],\n",
    "              optimizer= 'adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "S9-pR52ly6bW"
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# from math import log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7Nd2VhEjYOu",
    "outputId": "a14d35f9-c56d-488a-ae48-848e7871779a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.6838 - accuracy: 0.5466 - f1: 0.4669 - val_loss: 0.6505 - val_accuracy: 0.5441 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.00000, saving model to model.hdf5\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6762 - accuracy: 0.5553 - f1: 0.4659 - val_loss: 0.6484 - val_accuracy: 0.5655 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.00000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6682 - accuracy: 0.5665 - f1: 0.4675 - val_loss: 0.6462 - val_accuracy: 0.5910 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.00000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6610 - accuracy: 0.5752 - f1: 0.4663 - val_loss: 0.6437 - val_accuracy: 0.6263 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.00000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6544 - accuracy: 0.5819 - f1: 0.4619 - val_loss: 0.6411 - val_accuracy: 0.6803 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.00000\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6487 - accuracy: 0.5907 - f1: 0.4577 - val_loss: 0.6384 - val_accuracy: 0.7424 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.00000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6432 - accuracy: 0.6025 - f1: 0.4506 - val_loss: 0.6354 - val_accuracy: 0.8051 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.00000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6391 - accuracy: 0.6113 - f1: 0.4377 - val_loss: 0.6323 - val_accuracy: 0.8701 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.00000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6354 - accuracy: 0.6182 - f1: 0.4260 - val_loss: 0.6290 - val_accuracy: 0.9203 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.00000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6326 - accuracy: 0.6254 - f1: 0.4207 - val_loss: 0.6255 - val_accuracy: 0.9507 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.00000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6295 - accuracy: 0.6310 - f1: 0.4132 - val_loss: 0.6220 - val_accuracy: 0.9708 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.00000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6262 - accuracy: 0.6386 - f1: 0.4097 - val_loss: 0.6184 - val_accuracy: 0.9856 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.00000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6229 - accuracy: 0.6454 - f1: 0.4058 - val_loss: 0.6148 - val_accuracy: 0.9938 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.00000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6195 - accuracy: 0.6525 - f1: 0.4051 - val_loss: 0.6112 - val_accuracy: 0.9971 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.00000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6160 - accuracy: 0.6591 - f1: 0.4026 - val_loss: 0.6076 - val_accuracy: 0.9981 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.00000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6126 - accuracy: 0.6657 - f1: 0.4044 - val_loss: 0.6040 - val_accuracy: 0.9989 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.00000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6088 - accuracy: 0.6706 - f1: 0.4050 - val_loss: 0.6005 - val_accuracy: 0.9995 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.00000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.6059 - accuracy: 0.6749 - f1: 0.4050 - val_loss: 0.5970 - val_accuracy: 0.9998 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.00000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.6024 - accuracy: 0.6790 - f1: 0.4083 - val_loss: 0.5935 - val_accuracy: 0.9999 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.00000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5996 - accuracy: 0.6825 - f1: 0.4095 - val_loss: 0.5901 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.00000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5966 - accuracy: 0.6855 - f1: 0.4107 - val_loss: 0.5867 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.00000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.5939 - accuracy: 0.6890 - f1: 0.4138 - val_loss: 0.5834 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.00000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5908 - accuracy: 0.6931 - f1: 0.4168 - val_loss: 0.5801 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.00000\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5882 - accuracy: 0.6954 - f1: 0.4181 - val_loss: 0.5769 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.00000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5851 - accuracy: 0.7001 - f1: 0.4238 - val_loss: 0.5737 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.00000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5821 - accuracy: 0.7029 - f1: 0.4261 - val_loss: 0.5705 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.00000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5784 - accuracy: 0.7082 - f1: 0.4326 - val_loss: 0.5674 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.00000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.5753 - accuracy: 0.7104 - f1: 0.4352 - val_loss: 0.5643 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.00000\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5724 - accuracy: 0.7130 - f1: 0.4372 - val_loss: 0.5613 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.00000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5692 - accuracy: 0.7163 - f1: 0.4412 - val_loss: 0.5584 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.00000\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5654 - accuracy: 0.7191 - f1: 0.4451 - val_loss: 0.5555 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00031: val_f1 did not improve from 0.00000\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5622 - accuracy: 0.7215 - f1: 0.4478 - val_loss: 0.5528 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_f1 did not improve from 0.00000\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.5590 - accuracy: 0.7241 - f1: 0.4524 - val_loss: 0.5501 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_f1 did not improve from 0.00000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5548 - accuracy: 0.7277 - f1: 0.4603 - val_loss: 0.5475 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00034: val_f1 did not improve from 0.00000\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5513 - accuracy: 0.7294 - f1: 0.4630 - val_loss: 0.5450 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_f1 did not improve from 0.00000\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5471 - accuracy: 0.7333 - f1: 0.4716 - val_loss: 0.5426 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00036: val_f1 did not improve from 0.00000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.5433 - accuracy: 0.7370 - f1: 0.4795 - val_loss: 0.5402 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00037: val_f1 did not improve from 0.00000\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5394 - accuracy: 0.7386 - f1: 0.4847 - val_loss: 0.5380 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00038: val_f1 did not improve from 0.00000\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5348 - accuracy: 0.7430 - f1: 0.4951 - val_loss: 0.5359 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00039: val_f1 did not improve from 0.00000\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5307 - accuracy: 0.7456 - f1: 0.5035 - val_loss: 0.5338 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00040: val_f1 did not improve from 0.00000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.5266 - accuracy: 0.7490 - f1: 0.5116 - val_loss: 0.5318 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00041: val_f1 did not improve from 0.00000\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5224 - accuracy: 0.7536 - f1: 0.5251 - val_loss: 0.5298 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00042: val_f1 did not improve from 0.00000\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5179 - accuracy: 0.7573 - f1: 0.5360 - val_loss: 0.5279 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00043: val_f1 did not improve from 0.00000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5137 - accuracy: 0.7623 - f1: 0.5501 - val_loss: 0.5260 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00044: val_f1 did not improve from 0.00000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.5090 - accuracy: 0.7666 - f1: 0.5628 - val_loss: 0.5241 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00045: val_f1 did not improve from 0.00000\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.5047 - accuracy: 0.7707 - f1: 0.5743 - val_loss: 0.5222 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00046: val_f1 did not improve from 0.00000\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.5003 - accuracy: 0.7751 - f1: 0.5867 - val_loss: 0.5204 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00047: val_f1 did not improve from 0.00000\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4960 - accuracy: 0.7791 - f1: 0.5978 - val_loss: 0.5185 - val_accuracy: 1.0000 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00048: val_f1 did not improve from 0.00000\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4917 - accuracy: 0.7829 - f1: 0.6088 - val_loss: 0.5165 - val_accuracy: 0.9999 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00049: val_f1 did not improve from 0.00000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4874 - accuracy: 0.7859 - f1: 0.6172 - val_loss: 0.5145 - val_accuracy: 0.9999 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00050: val_f1 did not improve from 0.00000\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4833 - accuracy: 0.7893 - f1: 0.6263 - val_loss: 0.5125 - val_accuracy: 0.9999 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00051: val_f1 did not improve from 0.00000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4791 - accuracy: 0.7929 - f1: 0.6362 - val_loss: 0.5104 - val_accuracy: 0.9998 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00052: val_f1 did not improve from 0.00000\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4747 - accuracy: 0.7959 - f1: 0.6436 - val_loss: 0.5083 - val_accuracy: 0.9996 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00053: val_f1 did not improve from 0.00000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4706 - accuracy: 0.7988 - f1: 0.6511 - val_loss: 0.5061 - val_accuracy: 0.9994 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00054: val_f1 did not improve from 0.00000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4666 - accuracy: 0.8019 - f1: 0.6589 - val_loss: 0.5038 - val_accuracy: 0.9993 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00055: val_f1 did not improve from 0.00000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4629 - accuracy: 0.8041 - f1: 0.6641 - val_loss: 0.5015 - val_accuracy: 0.9991 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00056: val_f1 did not improve from 0.00000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4582 - accuracy: 0.8074 - f1: 0.6717 - val_loss: 0.4991 - val_accuracy: 0.9990 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00057: val_f1 did not improve from 0.00000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4546 - accuracy: 0.8102 - f1: 0.6780 - val_loss: 0.4967 - val_accuracy: 0.9989 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00058: val_f1 did not improve from 0.00000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4502 - accuracy: 0.8131 - f1: 0.6843 - val_loss: 0.4942 - val_accuracy: 0.9987 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00059: val_f1 did not improve from 0.00000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4468 - accuracy: 0.8153 - f1: 0.6890 - val_loss: 0.4918 - val_accuracy: 0.9986 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00060: val_f1 did not improve from 0.00000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4425 - accuracy: 0.8188 - f1: 0.6961 - val_loss: 0.4894 - val_accuracy: 0.9985 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00061: val_f1 did not improve from 0.00000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4381 - accuracy: 0.8210 - f1: 0.7003 - val_loss: 0.4871 - val_accuracy: 0.9984 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00062: val_f1 did not improve from 0.00000\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4343 - accuracy: 0.8246 - f1: 0.7072 - val_loss: 0.4848 - val_accuracy: 0.9982 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00063: val_f1 did not improve from 0.00000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4298 - accuracy: 0.8276 - f1: 0.7130 - val_loss: 0.4827 - val_accuracy: 0.9980 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00064: val_f1 did not improve from 0.00000\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4262 - accuracy: 0.8304 - f1: 0.7180 - val_loss: 0.4807 - val_accuracy: 0.9979 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00065: val_f1 did not improve from 0.00000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4219 - accuracy: 0.8335 - f1: 0.7241 - val_loss: 0.4788 - val_accuracy: 0.9977 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00066: val_f1 did not improve from 0.00000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4180 - accuracy: 0.8365 - f1: 0.7295 - val_loss: 0.4771 - val_accuracy: 0.9975 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00067: val_f1 did not improve from 0.00000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.4139 - accuracy: 0.8394 - f1: 0.7347 - val_loss: 0.4755 - val_accuracy: 0.9974 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00068: val_f1 did not improve from 0.00000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4097 - accuracy: 0.8428 - f1: 0.7410 - val_loss: 0.4740 - val_accuracy: 0.9973 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00069: val_f1 did not improve from 0.00000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4054 - accuracy: 0.8452 - f1: 0.7452 - val_loss: 0.4726 - val_accuracy: 0.9972 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00070: val_f1 did not improve from 0.00000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.4016 - accuracy: 0.8489 - f1: 0.7519 - val_loss: 0.4712 - val_accuracy: 0.9970 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00071: val_f1 did not improve from 0.00000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.3974 - accuracy: 0.8518 - f1: 0.7567 - val_loss: 0.4698 - val_accuracy: 0.9968 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00072: val_f1 did not improve from 0.00000\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 14s 14s/step - loss: 0.3929 - accuracy: 0.8546 - f1: 0.7619 - val_loss: 0.4685 - val_accuracy: 0.9968 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00073: val_f1 did not improve from 0.00000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.3891 - accuracy: 0.8578 - f1: 0.7674 - val_loss: 0.4671 - val_accuracy: 0.9966 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00074: val_f1 did not improve from 0.00000\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3849 - accuracy: 0.8608 - f1: 0.7727 - val_loss: 0.4656 - val_accuracy: 0.9964 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00075: val_f1 did not improve from 0.00000\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3805 - accuracy: 0.8632 - f1: 0.7771 - val_loss: 0.4641 - val_accuracy: 0.9962 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00076: val_f1 did not improve from 0.00000\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3761 - accuracy: 0.8665 - f1: 0.7827 - val_loss: 0.4625 - val_accuracy: 0.9961 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00077: val_f1 did not improve from 0.00000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3721 - accuracy: 0.8695 - f1: 0.7881 - val_loss: 0.4609 - val_accuracy: 0.9958 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00078: val_f1 did not improve from 0.00000\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.3681 - accuracy: 0.8723 - f1: 0.7927 - val_loss: 0.4592 - val_accuracy: 0.9957 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00079: val_f1 did not improve from 0.00000\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.3643 - accuracy: 0.8745 - f1: 0.7968 - val_loss: 0.4575 - val_accuracy: 0.9955 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00080: val_f1 did not improve from 0.00000\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3601 - accuracy: 0.8774 - f1: 0.8018 - val_loss: 0.4558 - val_accuracy: 0.9954 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00081: val_f1 did not improve from 0.00000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3559 - accuracy: 0.8805 - f1: 0.8071 - val_loss: 0.4541 - val_accuracy: 0.9952 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00082: val_f1 did not improve from 0.00000\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3519 - accuracy: 0.8831 - f1: 0.8116 - val_loss: 0.4523 - val_accuracy: 0.9951 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00083: val_f1 did not improve from 0.00000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3481 - accuracy: 0.8850 - f1: 0.8149 - val_loss: 0.4506 - val_accuracy: 0.9949 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00084: val_f1 did not improve from 0.00000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3441 - accuracy: 0.8880 - f1: 0.8201 - val_loss: 0.4487 - val_accuracy: 0.9947 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00085: val_f1 did not improve from 0.00000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3401 - accuracy: 0.8905 - f1: 0.8245 - val_loss: 0.4468 - val_accuracy: 0.9944 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00086: val_f1 did not improve from 0.00000\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.3364 - accuracy: 0.8926 - f1: 0.8280 - val_loss: 0.4448 - val_accuracy: 0.9941 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00087: val_f1 did not improve from 0.00000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3327 - accuracy: 0.8944 - f1: 0.8311 - val_loss: 0.4427 - val_accuracy: 0.9939 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00088: val_f1 did not improve from 0.00000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3289 - accuracy: 0.8967 - f1: 0.8352 - val_loss: 0.4404 - val_accuracy: 0.9938 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00089: val_f1 did not improve from 0.00000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 16s 16s/step - loss: 0.3253 - accuracy: 0.8988 - f1: 0.8385 - val_loss: 0.4381 - val_accuracy: 0.9936 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00090: val_f1 did not improve from 0.00000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.3217 - accuracy: 0.9010 - f1: 0.8422 - val_loss: 0.4359 - val_accuracy: 0.9934 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00091: val_f1 did not improve from 0.00000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3180 - accuracy: 0.9030 - f1: 0.8454 - val_loss: 0.4336 - val_accuracy: 0.9930 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00092: val_f1 did not improve from 0.00000\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3147 - accuracy: 0.9041 - f1: 0.8476 - val_loss: 0.4313 - val_accuracy: 0.9928 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00093: val_f1 did not improve from 0.00000\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3111 - accuracy: 0.9061 - f1: 0.8508 - val_loss: 0.4291 - val_accuracy: 0.9925 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00094: val_f1 did not improve from 0.00000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.3075 - accuracy: 0.9078 - f1: 0.8536 - val_loss: 0.4270 - val_accuracy: 0.9921 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00095: val_f1 did not improve from 0.00000\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3044 - accuracy: 0.9093 - f1: 0.8562 - val_loss: 0.4251 - val_accuracy: 0.9916 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00096: val_f1 did not improve from 0.00000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.3009 - accuracy: 0.9109 - f1: 0.8590 - val_loss: 0.4234 - val_accuracy: 0.9910 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00097: val_f1 did not improve from 0.00000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 14s 14s/step - loss: 0.2976 - accuracy: 0.9122 - f1: 0.8613 - val_loss: 0.4215 - val_accuracy: 0.9905 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00098: val_f1 did not improve from 0.00000\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.2944 - accuracy: 0.9133 - f1: 0.8632 - val_loss: 0.4194 - val_accuracy: 0.9898 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00099: val_f1 did not improve from 0.00000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.2914 - accuracy: 0.9143 - f1: 0.8649 - val_loss: 0.4171 - val_accuracy: 0.9893 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00100: val_f1 did not improve from 0.00000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 15s 15s/step - loss: 0.2886 - accuracy: 0.9158 - f1: 0.8674 - val_loss: 0.4145 - val_accuracy: 0.9887 - val_f1: 0.0000e+00\n",
      "\n",
      "Epoch 00101: val_f1 did not improve from 0.00000\n",
      "Epoch 00101: early stopping\n",
      "-----------------------------------------------------------------\n",
      "Training was completed in 1396.70 secs\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# # Define a learning rate decay method:\n",
    "# lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "#                              patience=1, verbose=1, \n",
    "#                              factor=0.5, min_lr=1e-8)\n",
    "# # Define Early Stopping:\n",
    "# early_stop = EarlyStopping(monitor='val_f1', min_delta=0.01, \n",
    "#                            patience=50, verbose=1, mode='auto',\n",
    "#                            baseline=0, restore_best_weights=True)\n",
    "check_point = ModelCheckpoint('model.hdf5', monitor=\"val_f1\", mode=\"max\",\n",
    "                              verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=100,verbose=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "classifier = model.fit(X_train_LSTM, y_train_LSTM,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0.0,\n",
    "                    validation_data=(X_valid_LSTM, y_valid_LSTM),\n",
    "                    shuffle=False,\n",
    "                    callbacks=[early_stop, check_point])\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a83KwP65jYOx",
    "outputId": "0e67c417-ba68-48c4-afa0-05b3bc9e9fc3"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-bed2cf9b6b41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluate the model:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m train_loss, train_acc = model.evaluate(X_train_LSTM, y_train_LSTM,\n\u001b[0m\u001b[0;32m      3\u001b[0m                                        batch_size=M_TRAIN, verbose=0)\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# test_loss, test_acc = model.evaluate(X_test_LSTM, y_test_LSTM,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#                                      batch_size=M_TEST, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Evaluate the model:\n",
    "train_loss, train_acc = model.evaluate(X_train_LSTM, y_train_LSTM,\n",
    "                                       batch_size=M_TRAIN, verbose=0)\n",
    "# test_loss, test_acc = model.evaluate(X_test_LSTM, y_test_LSTM,\n",
    "#                                      batch_size=M_TEST, verbose=0)\n",
    "# print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "# print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "# print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ovyuRgyguzVJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TC_H8xEjYOy",
    "outputId": "20e3534f-e5ff-4c60-9af7-954541ec86b0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat = model.predict_classes(X_train_LSTM, batch_size = M_TRAIN, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6yhQMJqjYO1",
    "outputId": "b6d3d236-cc73-4a8f-bd7b-35719ea30673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTobGwtmv_oy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2ZD8hL0vKD0",
    "outputId": "01a00b75-948a-4147-d756-b5ef5df6e2c3"
   },
   "outputs": [],
   "source": [
    "type(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9A5kKe8SxM3"
   },
   "outputs": [],
   "source": [
    "y_valid_1d = y_valid_LSTM.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1d = y_train_LSTM.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjJlJJWfuFBr",
    "outputId": "37a1541a-80d2-4109-8438-ef5eb3ad9d63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1880"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(0, len(yhat)):\n",
    "    if yhat[i] == y_1d[i] and yhat[i] == 1:\n",
    "        sum = sum + 1\n",
    "sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsmohXP587uM",
    "outputId": "95140a4f-bcbd-4f80-d2d8-f6e524364696"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in yhat:\n",
    "    if i == 1:\n",
    "        sum = sum + 1\n",
    "sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in y_valid_1d:\n",
    "    if i == 1:\n",
    "        sum = sum + 1\n",
    "sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLLfXvPy86sd",
    "outputId": "9212c178-28e9-4caa-b6f7-e684db04b4e0"
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in y_valid_1d:\n",
    "   if i == 1:\n",
    "    sum = sum + 1\n",
    "sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "hMx-H-FOjYPG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5601507845163318\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_1d, yhat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pB3ve6fujYPH",
    "outputId": "d0a8f3a6-ff79-4511-eace-e7c1cbb6145f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81    212070\n",
      "           1       0.94      0.02      0.04    101184\n",
      "\n",
      "    accuracy                           0.68    313254\n",
      "   macro avg       0.81      0.51      0.42    313254\n",
      "weighted avg       0.77      0.68      0.56    313254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_1d, yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94R_mkarvbPp"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yve0Q-DkjYPJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD_mkdecjYPS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mQoDiXYjYPT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "XdbNEUGDjYPU",
    "outputId": "79eebc18-d855-429e-adf4-5c396d286b16"
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYjGSTg-jYPU"
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdTskVNijYPV"
   },
   "outputs": [],
   "source": [
    "y_train['transformed'] = y_train.apply(lambda x: ''.join(x.astype(str)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDKsRCkujYPV"
   },
   "outputs": [],
   "source": [
    "le_y = preprocessing.LabelEncoder()\n",
    "y_train['encoded'] = le_y.fit_transform(y_train['transformed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sY4JsLpjYPX"
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7bpjREzjYPY"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(x_train['dayofweek'])\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeExXXRdjYPZ"
   },
   "outputs": [],
   "source": [
    "scale_list = ['absdif', 'dif', 'entropy', 'hurst', 'load', 'max', 'nonlinear', 'var']\n",
    "scaler = preprocessing.StandardScaler().fit(x_train[scale_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzXKfIJAjYPZ"
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ul0SZtNGjYPa"
   },
   "outputs": [],
   "source": [
    "x_train['dayofweek'] = le.transform(x_train['dayofweek'])\n",
    "x_train[scale_list] = scaler.transform(x_train[scale_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muldN2kjjYPb"
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tifRenVjYPd"
   },
   "outputs": [],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfJrlyDWjYPe"
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "x_train.plot(subplots=True,\n",
    "        layout=(6, 3),\n",
    "        figsize=(22,22),\n",
    "        fontsize=10, \n",
    "        linewidth=2,\n",
    "        sharex=False,\n",
    "        title='Visualization of the original Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FamrP5vVjYPf"
   },
   "source": [
    "The raw data contain stochastic time series. Predicting/ making classification based on stochastic variable values may force the model to learn the 'persistence' mode (i.e. yhat(t+1) = y(t)), resulting in little predictive power. Defining the model to predict (make classification from) the difference in values between the time steps rather than value itself, is a stronger test of its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-KaxalPjYPg"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.copy().pct_change(1)\n",
    "x_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZAjBpImjYPh"
   },
   "outputs": [],
   "source": [
    "x_train.fillna(method='bfill', inplace=True)\n",
    "x_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXPw4thTjYPi"
   },
   "outputs": [],
   "source": [
    "x_train.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb_1mS4njYPi"
   },
   "outputs": [],
   "source": [
    "X_train_final, X_valid, y_train_final, y_valid = train_test_split(x_train, y_train, test_size = 0.4, random_state = 42,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAMhXLdUjYPj"
   },
   "outputs": [],
   "source": [
    "# I want to use a T-mins window of input data for predicting target_class\n",
    "# It means I need to prepend (T-1) last train records to the 1st test window\n",
    "T = 30  # my choice of the timesteps window\n",
    "\n",
    "prepend_features = X_train_final.iloc[-(T-1):]\n",
    "X_valid = pd.concat([prepend_features, X_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_b7aBTHjYPk"
   },
   "outputs": [],
   "source": [
    "X_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRznP8zvjYPl"
   },
   "outputs": [],
   "source": [
    "X_train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGhSwmK0jYPm"
   },
   "outputs": [],
   "source": [
    "X_train_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8VmfvsjjYPm"
   },
   "outputs": [],
   "source": [
    "X_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYqC1BlYjYP0"
   },
   "outputs": [],
   "source": [
    "X_train_final.shape, X_valid.shape, y_train_final.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERVMnvftjYP1"
   },
   "source": [
    "Input data for the Keras LSTM layer has 3 dimensions: (M, T, N), where\n",
    "\n",
    "- M - number of examples (2D: sequences of timesteps x features),\n",
    "- T - sequence length (timesteps) and\n",
    "- N - number of features (input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrZGvPY0jYP2"
   },
   "outputs": [],
   "source": [
    "X_train_final.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1TdjIp6jYP2"
   },
   "outputs": [],
   "source": [
    "# Create sequences of T timesteps\n",
    "X_train_LSTM, y_train_LSTM = [], []\n",
    "for i in range(y_train_final.shape[0] - (T-1)):\n",
    "    X_train_LSTM.append(X_train_final.iloc[i:i+T].values)\n",
    "    y_train_LSTM.append(y_train_final.iloc[i + (T-1)])\n",
    "# X_train_LSTM, y_train_LSTM = np.array(X_train_LSTM), np.array(y_train_LSTM).reshape(-1,1)\n",
    "# print(f'Train data dimensions: {X_train_LSTM.shape}, {y_train_LSTM.shape}')\n",
    "\n",
    "# X_valid, y_test = [], []\n",
    "# for i in range(test_labels.shape[0]):\n",
    "#     X_test.append(scaled_test_features.iloc[i:i+T].values)\n",
    "#     y_test.append(test_labels.iloc[i])\n",
    "# X_test, y_test = np.array(X_test), np.array(y_test).reshape(-1,1)  \n",
    "\n",
    "# print(f'Test data dimensions: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Csrbo2CjYP3"
   },
   "outputs": [],
   "source": [
    "X_train_LSTM\n",
    "X_train_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeInIx2BjYP4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wmFsha9jYP4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sniJu7zfjYP5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xKMWTn2jYP5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split train and validation data\n",
    "train_features = x_train.loc['2012-01-02':'2016-12-31']\n",
    "train_labels = df.loc['2012-01-02':'2016-12-31', 'target_class']\n",
    "\n",
    "test_features = df_transform.loc['2017-01-02':'2018-06-19']\n",
    "test_labels = df.loc['2017-01-02':'2018-06-19', 'target_class']\n",
    "\n",
    "# I want to use a T-days window of input data for predicting target_class\n",
    "# It means I need to prepend (T-1) last train records to the 1st test window\n",
    "T = 45  # my choice of the timesteps window\n",
    "\n",
    "prepend_features = train_features.iloc[-(T-1):]\n",
    "test_features = pd.concat([prepend_features, test_features], axis=0)\n",
    "\n",
    "train_features.shape, train_labels.shape, test_features.shape, test_labels.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bản sao của Bản sao của Thai Nguyen LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
