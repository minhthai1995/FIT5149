{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112bb3bf",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb1a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "import tsfresh\n",
    "from tsfresh.feature_extraction import EfficientFCParameters, MinimalFCParameters, ComprehensiveFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import f1_score\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5e7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = '../FIT5149_A2_data/train_data_withlabels.csv'\n",
    "TEST_DATA_DIR = \"../FIT5149_A2_data/train_data_withlabels.csv\"\n",
    "MODEL_DIR = \"./bestf1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6035432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html\n",
    "# Compute the one-dimensional discrete Fourier Transform for real input.\n",
    "# This function computes the one-dimensional n-point discrete Fourier Transform (DFT)\n",
    "# of a real-valued array by means of an efficient algorithm called the Fast Fourier Transform (FFT).\n",
    "\n",
    "def absfft(x):\n",
    "    return np.abs(np.fft.rfft(x))\n",
    "\n",
    "# Helper function, used in building training and validating datasets\n",
    "def build_datasets(data, target, train_size, valid_pct = 0.2, seed = None):\n",
    "    x, x_fft = data # split the data in to raw and FFT data\n",
    "    idx = np.arange(train_size) # Create a list of indexes\n",
    "#     train_idx, val_idx = train_test_split(idx, test_size = valid_pct, random_state = seed)\n",
    "    # Start spliting the data into training and validating parts using the validate percentage (valid_pct) value\n",
    "    train_idx, val_idx = idx[round(train_size * valid_pct):], idx[:round(train_size * valid_pct)]\n",
    "    # Build the train data, which include raw x, FFT x, and target y \n",
    "    train_ds = TensorDataset(torch.tensor(x[:train_size][train_idx]).float(),\n",
    "                            torch.tensor(x_fft[:train_size][train_idx]).float(),\n",
    "                            torch.tensor(target[:train_size][train_idx]).long())\n",
    "    print(\"There are\",len(set(target[:train_size][train_idx])),\"class in training data\")\n",
    "    # Build the validating data, which include raw x, FFT x, and target y \n",
    "    val_ds = TensorDataset(torch.tensor(x[:train_size][val_idx]).float(),\n",
    "                            torch.tensor(x_fft[:train_size][val_idx]).float(),\n",
    "                            torch.tensor(target[:train_size][val_idx]).long())\n",
    "    return train_ds, val_ds \n",
    "\n",
    "# Helper function, \n",
    "def build_loaders(data, batch_size = 128, jobs = 8):\n",
    "    train_ds, valid_ds = data\n",
    "    # Build a train dataloader\n",
    "    train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = False, num_workers = jobs)\n",
    "    # Build a test dataloader\n",
    "    valid_dl = DataLoader(valid_ds, batch_size = batch_size, shuffle = False, num_workers = jobs)\n",
    "    return train_dl, valid_dl \n",
    "\n",
    "# https://www.kaggle.com/purplejester/pytorch-deep-time-series-classification\n",
    "class _SepConv1d(nn.Module):\n",
    "    \"\"\"A simple separable convolution implementation.\n",
    "    \n",
    "    The separable convlution is a method to reduce number of the parameters \n",
    "    in the deep learning network for slight decrease in predictions quality.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, no, kernel, stride, pad):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(ni, ni, kernel, stride, padding=pad, groups=ni)\n",
    "        self.pointwise = nn.Conv1d(ni, no, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))\n",
    "    \n",
    "class SepConv1d(nn.Module):\n",
    "    \"\"\"Implementes a 1-d convolution with 'batteries included'.\n",
    "    \n",
    "    The module adds (optionally) activation function and dropout \n",
    "    layers right after a separable convolution layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, no, kernel, stride, pad, \n",
    "                 drop=None, bn=False,\n",
    "                 activ=lambda: nn.ReLU()):\n",
    "    \n",
    "        super().__init__()\n",
    "        # Check the drop out rate\n",
    "        assert drop is None or (0.0 < drop < 1.0)\n",
    "        # Build a separable convolution layer, using channel_in, channel_out, kernel size, stride, and padding size \n",
    "        layers = [_SepConv1d(ni, no, kernel, stride, pad)]\n",
    "        # Add an activation function\n",
    "        if activ:\n",
    "            layers.append(activ())\n",
    "        # Apply batch normalization if required\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm1d(no))\n",
    "        # Apply drop out rate to prevent the model from overfitting\n",
    "        if drop is not None:\n",
    "            layers.append(nn.Dropout(drop))\n",
    "        # chain all of the layers into one object\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    # Define forward function\n",
    "    def forward(self, x): \n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Helper function, used to build flatten layers\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "    # Define forward function, which flatten the input into 1 dimension, or 2 dimension which is [batchsize, -1]\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)\n",
    "    \n",
    "# Helper function to print the shape of a layer's output, this function is useful when building and debugging \n",
    "# models\n",
    "class PrintSize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintSize, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "# model building, which includes 2 branches, one brach for raw input with 30 time steps, one branch for \n",
    "# FFT input with 30/2 + 1 = 16 timesteps.\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, raw_ni, fft_ni, no, drop=.5):\n",
    "        super().__init__()\n",
    "        #PKS [[4,8,2],[1,3,1],[3,8,2],[1,3,1],[5,8,2],[1,3,1],[2,8,2]]\n",
    "        self.raw = nn.Sequential( \n",
    "            #         (in ,out ,kernel, stride, pad)\n",
    "            SepConv1d(raw_ni,  32, 8, 2, 4, drop=drop),\n",
    "            SepConv1d(    32,  64, 8, 2, 3, drop=drop),\n",
    "            SepConv1d(    64, 128, 8, 2, 5, drop=drop),\n",
    "            SepConv1d(   128, 256, 8, 2, 2, drop=drop),\n",
    "            Flatten(),\n",
    "#             PrintSize(),\n",
    "            nn.Dropout(drop), nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Dropout(drop), nn.Linear(256, 64), nn.ReLU())\n",
    "        #PKS [[3,8,2],[1,3,1],[5,8,2],[1,3,1],[4,8,2],[1,3,1],[3,8,2]]\n",
    "        self.fft = nn.Sequential(\n",
    "            SepConv1d(fft_ni,  32, 8, 2, 3, drop=drop),\n",
    "            SepConv1d(    32,  64, 8, 2, 5, drop=drop),\n",
    "            SepConv1d(    64, 128, 8, 2, 4, drop=drop),\n",
    "            SepConv1d(   128, 128, 8, 2, 5, drop=drop),\n",
    "            SepConv1d(   128, 256, 8, 2, 3, drop=drop),\n",
    "            Flatten(),\n",
    "#             PrintSize(),\n",
    "            nn.Dropout(drop), nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Dropout(drop), nn.Linear(256, 64), nn.ReLU())\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.ReLU(inplace=True), nn.Linear(64, no))\n",
    "        self.init_weights(nn.init.kaiming_normal_)\n",
    "        \n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        def init(m): \n",
    "            for child in m.children():\n",
    "                if isinstance(child, nn.Conv1d):\n",
    "                    # Fills the input Tensor with values according to the method described in Delving deep into \n",
    "                    # rectifiers: Surpassing human-level performance on \n",
    "                    # ImageNet classification - He, K. et al. (2015), using a normal distribution\n",
    "                    init_fn(child.weights)\n",
    "        init(self)\n",
    "        \n",
    "    # Define a forward function for the model\n",
    "    def forward(self, t_raw, t_fft):\n",
    "        # raw x branch\n",
    "        raw_out = self.raw(t_raw)\n",
    "        # fft x branch\n",
    "        fft_out = self.fft(t_fft)\n",
    "        # concat 2 branches into one\n",
    "        t_in = torch.cat([raw_out, fft_out], dim=1)\n",
    "        # push it through fully connected layers to get the output\n",
    "        out = self.out(t_in)\n",
    "        return out\n",
    "    \n",
    "# https://github.com/gokulprasadthekkel/pytorch-multi-class-focal-loss/blob/master/focal_loss.py\n",
    "# Helper function, implementation of focal loss in order to solve class imbalance problem\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9a9f478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>load</th>\n",
       "      <th>ac</th>\n",
       "      <th>ev</th>\n",
       "      <th>oven</th>\n",
       "      <th>wash</th>\n",
       "      <th>dryer</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dif</th>\n",
       "      <th>absdif</th>\n",
       "      <th>max</th>\n",
       "      <th>var</th>\n",
       "      <th>entropy</th>\n",
       "      <th>nonlinear</th>\n",
       "      <th>hurst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105569</td>\n",
       "      <td>2.245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.987</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.074549</td>\n",
       "      <td>0.678886</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>0.994071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105570</td>\n",
       "      <td>2.259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.172867</td>\n",
       "      <td>0.667450</td>\n",
       "      <td>0.054829</td>\n",
       "      <td>0.994154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105571</td>\n",
       "      <td>2.269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.270112</td>\n",
       "      <td>0.647777</td>\n",
       "      <td>0.056991</td>\n",
       "      <td>0.994220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105572</td>\n",
       "      <td>2.268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.303763</td>\n",
       "      <td>0.629227</td>\n",
       "      <td>0.057606</td>\n",
       "      <td>0.994150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105573</td>\n",
       "      <td>2.270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.215</td>\n",
       "      <td>3.302744</td>\n",
       "      <td>0.621295</td>\n",
       "      <td>0.082640</td>\n",
       "      <td>0.994041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417715</th>\n",
       "      <td>523284</td>\n",
       "      <td>2.543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417716</th>\n",
       "      <td>523285</td>\n",
       "      <td>2.417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417717</th>\n",
       "      <td>523286</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-1.418</td>\n",
       "      <td>1.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417718</th>\n",
       "      <td>523287</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417719</th>\n",
       "      <td>523288</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>Tue</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417720 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id   load  ac  ev  oven  wash  dryer  hourofday dayofweek    dif  \\\n",
       "0       105569  2.245   0   0     0     0      0          0       Sun  0.987   \n",
       "1       105570  2.259   0   0     0     0      0          0       Sun  0.014   \n",
       "2       105571  2.269   0   0     0     0      0          0       Sun  0.010   \n",
       "3       105572  2.268   0   0     0     0      0          0       Sun -0.001   \n",
       "4       105573  2.270   0   0     0     0      0          0       Sun  0.002   \n",
       "...        ...    ...  ..  ..   ...   ...    ...        ...       ...    ...   \n",
       "417715  523284  2.543   0   0     0     0      0         21       Tue -0.003   \n",
       "417716  523285  2.417   0   0     0     0      0         21       Tue -0.126   \n",
       "417717  523286  0.999   0   0     0     0      0         21       Tue -1.418   \n",
       "417718  523287  0.966   0   0     0     0      0         21       Tue -0.033   \n",
       "417719  523288  0.964   0   0     0     0      0         21       Tue -0.002   \n",
       "\n",
       "        absdif    max       var   entropy  nonlinear     hurst  \n",
       "0        0.987  6.215  3.074549  0.678886   0.052903  0.994071  \n",
       "1        0.014  6.215  3.172867  0.667450   0.054829  0.994154  \n",
       "2        0.010  6.215  3.270112  0.647777   0.056991  0.994220  \n",
       "3        0.001  6.215  3.303763  0.629227   0.057606  0.994150  \n",
       "4        0.002  6.215  3.302744  0.621295   0.082640  0.994041  \n",
       "...        ...    ...       ...       ...        ...       ...  \n",
       "417715   0.003  0.000  0.000000  0.000000   0.000000  0.000000  \n",
       "417716   0.126  0.000  0.000000  0.000000   0.000000  0.000000  \n",
       "417717   1.418  0.000  0.000000  0.000000   0.000000  0.000000  \n",
       "417718   0.033  0.000  0.000000  0.000000   0.000000  0.000000  \n",
       "417719   0.002  0.000  0.000000  0.000000   0.000000  0.000000  \n",
       "\n",
       "[417720 rows x 16 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the testing data\n",
    "x_test = pd.read_csv(TEST_DATA_DIR)\n",
    "x_test.rename(columns={'Unnamed: 0': 'id'}, inplace= 1) # Rename \n",
    "x_test['id'] = x_test['id'] + 28 # Increase the ID column by 28 for later rolling\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf6a84f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Label encoder and Standard scaler from training data\n",
    "data = pd.read_csv(TRAIN_DATA_DIR)\n",
    "data.rename(columns={\"Unnamed: 0\":\"id\"}, inplace= True)\n",
    "x_train = data.loc[:,data.columns.difference(['ac', 'ev', 'oven', 'wash', 'dryer'])]\n",
    "y_train = data[['ac', 'ev', 'oven', 'wash', 'dryer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afd95f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fri', 'Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder() # Create a label encoder for variable \"dayofweeek\"\n",
    "le.fit(x_train['dayofweek']) # Fit the values\n",
    "x_train['dayofweek'] = le.transform(x_train['dayofweek']) # Perform transformation\n",
    "x_test['dayofweek'] = le.transform(x_test['dayofweek'])\n",
    "\n",
    "le.classes_ # have a look at the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32743ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scale_list = ['absdif', 'dayofweek', 'dif', 'entropy', 'hourofday', 'hurst', 'load', 'max', 'nonlinear', 'var']\n",
    "scaler = preprocessing.StandardScaler().fit(x_train[scale_list]) # Fit the data values\n",
    "x_train[scale_list] = scaler.transform(x_train[scale_list]) # Perform scaling\n",
    "x_test[scale_list] = scaler.transform(x_test[scale_list]) # Perform scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2401a1",
   "metadata": {},
   "source": [
    "#### Get the y_train label encoder for later part of predictions decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfa77660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-16ec565c3dc2>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train['transformed'] = y_train.apply(lambda x: ''.join(x.astype(str)),axis = 1)\n",
      "<ipython-input-22-16ec565c3dc2>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train['encoded'] = le_y.fit_transform(y_train['transformed'])\n"
     ]
    }
   ],
   "source": [
    "# concatenate the values of the 5 appliances into a string, for example 0,1,0,0,1 will be 01001\n",
    "y_train['transformed'] = y_train.apply(lambda x: ''.join(x.astype(str)),axis = 1)\n",
    "# create a label encoder for y_train\n",
    "le_y = preprocessing.LabelEncoder()\n",
    "# fit and transform the values of the concatenated values \n",
    "y_train['encoded'] = le_y.fit_transform(y_train['transformed']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47d08b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the design choice is to use 30 minutes at each time-step, therefore each of the first 29 observations \n",
    "# will not have enough 29 observations before it in order to create a series of 30 observations.\n",
    "# Thus I will add 29 copies of the first observation to the top of the testing dataframe.\n",
    "extra_len = pd.DataFrame(x_test.iloc[0:1,:].values.repeat(29, axis = 0), columns=x_test.columns)\n",
    "extra_len['id'] = list(range(105569-29,105569))\n",
    "\n",
    "# concatenate the testing and the additional data\n",
    "x_test = pd.concat([extra_len, x_test])\n",
    "x_test['dummy_id'] = 1 # Create a dummy ID column for rolling\n",
    "x_test.reset_index(drop= True, inplace=True) # Reset index\n",
    "x_test = x_test[['id', 'load', 'absdif', 'dayofweek', 'dif', 'entropy', 'hourofday', 'hurst', \n",
    "                 'max', 'nonlinear', 'var','dummy_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0110b0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>load</th>\n",
       "      <th>absdif</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dif</th>\n",
       "      <th>entropy</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>hurst</th>\n",
       "      <th>max</th>\n",
       "      <th>nonlinear</th>\n",
       "      <th>var</th>\n",
       "      <th>dummy_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105540</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105541</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105542</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105543</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105544</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105545</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>105546</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>105547</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>105548</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>105549</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>105550</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>105551</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>105552</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>105553</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>105554</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>105555</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>105556</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>105557</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>105558</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>105559</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>105560</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>105561</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>105562</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>105563</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>105564</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>105565</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>105566</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>105567</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>105568</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>105569</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>1.633999</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.859011</td>\n",
       "      <td>-0.306037</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325913</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.542338</td>\n",
       "      <td>0.673127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>105570</td>\n",
       "      <td>0.039319</td>\n",
       "      <td>-0.287489</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.026370</td>\n",
       "      <td>-0.427219</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.327174</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.541600</td>\n",
       "      <td>0.728126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>105571</td>\n",
       "      <td>0.044609</td>\n",
       "      <td>-0.295388</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>-0.635696</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.328183</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.540772</td>\n",
       "      <td>0.782525</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>105572</td>\n",
       "      <td>0.044080</td>\n",
       "      <td>-0.313161</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.832269</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.327108</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.540536</td>\n",
       "      <td>0.801349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>105573</td>\n",
       "      <td>0.045138</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>-0.916324</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325441</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.530947</td>\n",
       "      <td>0.800779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>105574</td>\n",
       "      <td>0.039319</td>\n",
       "      <td>-0.293413</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.020717</td>\n",
       "      <td>-0.856331</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.324612</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.528210</td>\n",
       "      <td>0.798270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>105575</td>\n",
       "      <td>0.034559</td>\n",
       "      <td>-0.297363</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.016950</td>\n",
       "      <td>-0.716822</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.323697</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.529938</td>\n",
       "      <td>0.793166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>105576</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>-0.313161</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>-0.591830</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.304998</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.426930</td>\n",
       "      <td>0.765110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>105577</td>\n",
       "      <td>0.041964</td>\n",
       "      <td>-0.289464</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.024487</td>\n",
       "      <td>-0.552794</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.301508</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.419911</td>\n",
       "      <td>0.745114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>105578</td>\n",
       "      <td>0.048840</td>\n",
       "      <td>-0.289464</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.024487</td>\n",
       "      <td>-0.612073</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.298972</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.422190</td>\n",
       "      <td>0.741137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>105579</td>\n",
       "      <td>0.064709</td>\n",
       "      <td>-0.255892</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.056506</td>\n",
       "      <td>-0.734174</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.299238</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.378153</td>\n",
       "      <td>0.739002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>105580</td>\n",
       "      <td>0.105437</td>\n",
       "      <td>-0.163076</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.145030</td>\n",
       "      <td>-0.847520</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.298662</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.321732</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>105581</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>-0.873242</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.296953</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.254854</td>\n",
       "      <td>0.707632</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>105582</td>\n",
       "      <td>0.135587</td>\n",
       "      <td>-0.198622</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.111128</td>\n",
       "      <td>-0.789069</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.304879</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.403707</td>\n",
       "      <td>0.683860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>105583</td>\n",
       "      <td>0.139818</td>\n",
       "      <td>-0.299338</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.015069</td>\n",
       "      <td>-0.654897</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.313861</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.484818</td>\n",
       "      <td>0.698237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>105584</td>\n",
       "      <td>0.135058</td>\n",
       "      <td>-0.297363</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.016950</td>\n",
       "      <td>-0.557264</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.316627</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.461231</td>\n",
       "      <td>0.720795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>105585</td>\n",
       "      <td>0.137174</td>\n",
       "      <td>-0.307237</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>-0.548603</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.318485</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.441474</td>\n",
       "      <td>0.736902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>105586</td>\n",
       "      <td>0.126595</td>\n",
       "      <td>-0.275640</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.037669</td>\n",
       "      <td>-0.625789</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.319069</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.444668</td>\n",
       "      <td>0.739870</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>105587</td>\n",
       "      <td>0.127124</td>\n",
       "      <td>-0.313161</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>-0.734575</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.319522</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.442053</td>\n",
       "      <td>0.742952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>105588</td>\n",
       "      <td>0.130826</td>\n",
       "      <td>-0.301312</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>-0.797098</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.319934</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.443336</td>\n",
       "      <td>0.746444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>105589</td>\n",
       "      <td>0.130298</td>\n",
       "      <td>-0.313161</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.760468</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.320170</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.446212</td>\n",
       "      <td>0.747339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>105590</td>\n",
       "      <td>0.123421</td>\n",
       "      <td>-0.289464</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.024484</td>\n",
       "      <td>-0.651261</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.320295</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.445838</td>\n",
       "      <td>0.747163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>105591</td>\n",
       "      <td>0.124479</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>-0.553929</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.320382</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.445543</td>\n",
       "      <td>0.747723</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>105592</td>\n",
       "      <td>0.622215</td>\n",
       "      <td>1.543158</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>1.772370</td>\n",
       "      <td>-0.539035</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.320401</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.445584</td>\n",
       "      <td>0.747958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>105593</td>\n",
       "      <td>2.106429</td>\n",
       "      <td>5.226174</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>5.285089</td>\n",
       "      <td>-0.629372</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.320368</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.445073</td>\n",
       "      <td>0.748578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>105594</td>\n",
       "      <td>2.126000</td>\n",
       "      <td>-0.242068</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.069691</td>\n",
       "      <td>-0.799598</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.320266</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.445211</td>\n",
       "      <td>0.749196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>105595</td>\n",
       "      <td>2.131818</td>\n",
       "      <td>-0.293413</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.020720</td>\n",
       "      <td>-0.975499</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.320066</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.446560</td>\n",
       "      <td>0.749963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>105596</td>\n",
       "      <td>2.127587</td>\n",
       "      <td>-0.299338</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>-1.054998</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.319717</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.446789</td>\n",
       "      <td>0.749033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>105597</td>\n",
       "      <td>2.123355</td>\n",
       "      <td>-0.299338</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>-0.979371</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.319264</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.447078</td>\n",
       "      <td>0.747831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>105598</td>\n",
       "      <td>2.112247</td>\n",
       "      <td>-0.273665</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.039552</td>\n",
       "      <td>-0.807754</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.319330</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.436223</td>\n",
       "      <td>0.746745</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>105599</td>\n",
       "      <td>2.114892</td>\n",
       "      <td>-0.305262</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.009419</td>\n",
       "      <td>-0.658369</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.322090</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.422531</td>\n",
       "      <td>0.752675</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>105600</td>\n",
       "      <td>2.100082</td>\n",
       "      <td>-0.259841</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.052737</td>\n",
       "      <td>-0.607730</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.321828</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>0.774955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>105601</td>\n",
       "      <td>2.093734</td>\n",
       "      <td>-0.291438</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.022601</td>\n",
       "      <td>-0.665103</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.322164</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.419278</td>\n",
       "      <td>0.796737</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>105602</td>\n",
       "      <td>1.710780</td>\n",
       "      <td>1.114625</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-1.363649</td>\n",
       "      <td>-0.794700</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.322485</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.416510</td>\n",
       "      <td>0.818271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>105603</td>\n",
       "      <td>1.444192</td>\n",
       "      <td>0.680167</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.949280</td>\n",
       "      <td>-0.930708</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.322791</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.413729</td>\n",
       "      <td>0.839450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>105604</td>\n",
       "      <td>1.426208</td>\n",
       "      <td>-0.247993</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.064038</td>\n",
       "      <td>-0.995331</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.323079</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.409950</td>\n",
       "      <td>0.860231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>105605</td>\n",
       "      <td>1.421448</td>\n",
       "      <td>-0.297363</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.016950</td>\n",
       "      <td>-0.955258</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.323346</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.405928</td>\n",
       "      <td>0.880492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>105606</td>\n",
       "      <td>0.162034</td>\n",
       "      <td>4.386881</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-4.484601</td>\n",
       "      <td>-0.856938</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.323595</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.402111</td>\n",
       "      <td>0.900252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>105607</td>\n",
       "      <td>-0.116190</td>\n",
       "      <td>0.723613</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.990717</td>\n",
       "      <td>-0.779326</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.323833</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.398672</td>\n",
       "      <td>0.919617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>105608</td>\n",
       "      <td>-0.498087</td>\n",
       "      <td>1.110675</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-1.359882</td>\n",
       "      <td>-0.772474</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.324060</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.395011</td>\n",
       "      <td>0.938524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>105609</td>\n",
       "      <td>-0.580602</td>\n",
       "      <td>-0.007066</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.293824</td>\n",
       "      <td>-0.840591</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.324277</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.392546</td>\n",
       "      <td>0.956747</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>105610</td>\n",
       "      <td>-0.551510</td>\n",
       "      <td>-0.206522</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.103594</td>\n",
       "      <td>-0.939862</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.324514</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.393460</td>\n",
       "      <td>0.974551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>105611</td>\n",
       "      <td>-0.553097</td>\n",
       "      <td>-0.309212</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.005649</td>\n",
       "      <td>-1.006108</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.324730</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.387393</td>\n",
       "      <td>0.992301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>105612</td>\n",
       "      <td>-0.552039</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>-0.993884</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.324966</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.386559</td>\n",
       "      <td>1.009542</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>105613</td>\n",
       "      <td>-0.553097</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>-0.918583</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325175</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.380914</td>\n",
       "      <td>1.026159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>105614</td>\n",
       "      <td>-0.611281</td>\n",
       "      <td>-0.097907</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.207183</td>\n",
       "      <td>-0.843617</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325368</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.372338</td>\n",
       "      <td>1.042196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>105615</td>\n",
       "      <td>-0.626620</td>\n",
       "      <td>-0.257867</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.054620</td>\n",
       "      <td>-0.828263</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.325401</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.363473</td>\n",
       "      <td>1.057591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>105616</td>\n",
       "      <td>-0.598057</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.101710</td>\n",
       "      <td>-0.893496</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.315991</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.425083</td>\n",
       "      <td>1.060575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>105617</td>\n",
       "      <td>-0.622389</td>\n",
       "      <td>-0.224295</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.086639</td>\n",
       "      <td>-1.017953</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.304252</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.267211</td>\n",
       "      <td>1.116698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>105618</td>\n",
       "      <td>-0.612339</td>\n",
       "      <td>-0.277615</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.035788</td>\n",
       "      <td>-1.137791</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.307336</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>1.171829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>105619</td>\n",
       "      <td>-0.583776</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.101710</td>\n",
       "      <td>-1.173854</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.309956</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.188556</td>\n",
       "      <td>1.225095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>105620</td>\n",
       "      <td>-0.586949</td>\n",
       "      <td>-0.303287</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.011300</td>\n",
       "      <td>-1.090888</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.311930</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.115798</td>\n",
       "      <td>1.276644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>105621</td>\n",
       "      <td>-0.595941</td>\n",
       "      <td>-0.281564</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.032018</td>\n",
       "      <td>-0.939486</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.313748</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.009663</td>\n",
       "      <td>1.326070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>105622</td>\n",
       "      <td>-0.593826</td>\n",
       "      <td>-0.307237</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>-0.808089</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.321475</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.150362</td>\n",
       "      <td>1.373860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>105623</td>\n",
       "      <td>-0.599644</td>\n",
       "      <td>-0.293413</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.020717</td>\n",
       "      <td>-0.754436</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.338758</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.476507</td>\n",
       "      <td>1.414337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>105624</td>\n",
       "      <td>-0.596999</td>\n",
       "      <td>-0.305262</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.009419</td>\n",
       "      <td>-0.785236</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.337693</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.491593</td>\n",
       "      <td>1.339442</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>105625</td>\n",
       "      <td>-0.600173</td>\n",
       "      <td>-0.303287</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.011300</td>\n",
       "      <td>-0.873291</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.336793</td>\n",
       "      <td>1.050126</td>\n",
       "      <td>-0.503546</td>\n",
       "      <td>1.261359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>105626</td>\n",
       "      <td>-0.600173</td>\n",
       "      <td>-0.315136</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.944305</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.336293</td>\n",
       "      <td>1.046372</td>\n",
       "      <td>-0.512004</td>\n",
       "      <td>1.181862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>105627</td>\n",
       "      <td>-0.598057</td>\n",
       "      <td>-0.307237</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>-0.917153</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.334494</td>\n",
       "      <td>1.042618</td>\n",
       "      <td>-0.516407</td>\n",
       "      <td>1.101911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>105628</td>\n",
       "      <td>-0.601760</td>\n",
       "      <td>-0.301312</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.013183</td>\n",
       "      <td>-0.779301</td>\n",
       "      <td>-1.659524</td>\n",
       "      <td>0.331873</td>\n",
       "      <td>1.035110</td>\n",
       "      <td>-0.514610</td>\n",
       "      <td>0.997109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>105629</td>\n",
       "      <td>-0.602818</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>-0.601494</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.316616</td>\n",
       "      <td>1.035110</td>\n",
       "      <td>-0.546609</td>\n",
       "      <td>0.866342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>105630</td>\n",
       "      <td>-0.605991</td>\n",
       "      <td>-0.303287</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.011300</td>\n",
       "      <td>-0.476548</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.310773</td>\n",
       "      <td>1.021971</td>\n",
       "      <td>-0.560972</td>\n",
       "      <td>0.732682</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>105631</td>\n",
       "      <td>-0.603347</td>\n",
       "      <td>-0.305262</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.009419</td>\n",
       "      <td>-0.444758</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.303467</td>\n",
       "      <td>1.016340</td>\n",
       "      <td>-0.525110</td>\n",
       "      <td>0.606670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>105632</td>\n",
       "      <td>-0.605991</td>\n",
       "      <td>-0.305262</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.009416</td>\n",
       "      <td>-0.499406</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.300038</td>\n",
       "      <td>0.676608</td>\n",
       "      <td>-0.542625</td>\n",
       "      <td>0.473411</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>105633</td>\n",
       "      <td>-0.608107</td>\n",
       "      <td>-0.307237</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.007533</td>\n",
       "      <td>-0.591366</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.296517</td>\n",
       "      <td>0.440109</td>\n",
       "      <td>-0.544921</td>\n",
       "      <td>0.382290</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>105634</td>\n",
       "      <td>-0.609165</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>-0.658119</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.291377</td>\n",
       "      <td>0.424155</td>\n",
       "      <td>-0.547757</td>\n",
       "      <td>0.315007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>105635</td>\n",
       "      <td>-0.609165</td>\n",
       "      <td>-0.315136</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.664700</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.284638</td>\n",
       "      <td>0.419932</td>\n",
       "      <td>-0.548959</td>\n",
       "      <td>0.244786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>105636</td>\n",
       "      <td>-0.609165</td>\n",
       "      <td>-0.315136</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.621536</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.306666</td>\n",
       "      <td>0.388493</td>\n",
       "      <td>-0.476350</td>\n",
       "      <td>0.170412</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>105637</td>\n",
       "      <td>-0.610223</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.003766</td>\n",
       "      <td>-0.566331</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.308951</td>\n",
       "      <td>0.388493</td>\n",
       "      <td>-0.517800</td>\n",
       "      <td>0.173902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>105638</td>\n",
       "      <td>-0.609165</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>-0.536985</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.311812</td>\n",
       "      <td>0.388493</td>\n",
       "      <td>-0.482892</td>\n",
       "      <td>0.180724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>105639</td>\n",
       "      <td>-0.603876</td>\n",
       "      <td>-0.295388</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>-0.552604</td>\n",
       "      <td>-1.515023</td>\n",
       "      <td>0.312346</td>\n",
       "      <td>0.388493</td>\n",
       "      <td>-0.464523</td>\n",
       "      <td>0.184404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      load    absdif  dayofweek       dif   entropy  hourofday  \\\n",
       "0   105540  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "1   105541  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "2   105542  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "3   105543  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "4   105544  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "5   105545  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "6   105546  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "7   105547  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "8   105548  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "9   105549  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "10  105550  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "11  105551  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "12  105552  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "13  105553  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "14  105554  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "15  105555  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "16  105556  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "17  105557  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "18  105558  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "19  105559  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "20  105560  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "21  105561  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "22  105562  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "23  105563  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "24  105564  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "25  105565  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "26  105566  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "27  105567  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "28  105568  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "29  105569  0.031914  1.633999  -0.006818  1.859011 -0.306037  -1.659524   \n",
       "30  105570  0.039319 -0.287489  -0.006818  0.026370 -0.427219  -1.659524   \n",
       "31  105571  0.044609 -0.295388  -0.006818  0.018836 -0.635696  -1.659524   \n",
       "32  105572  0.044080 -0.313161  -0.006818 -0.001882 -0.832269  -1.659524   \n",
       "33  105573  0.045138 -0.311186  -0.006818  0.003768 -0.916324  -1.659524   \n",
       "34  105574  0.039319 -0.293413  -0.006818 -0.020717 -0.856331  -1.659524   \n",
       "35  105575  0.034559 -0.297363  -0.006818 -0.016950 -0.716822  -1.659524   \n",
       "36  105576  0.035088 -0.313161  -0.006818  0.001885 -0.591830  -1.659524   \n",
       "37  105577  0.041964 -0.289464  -0.006818  0.024487 -0.552794  -1.659524   \n",
       "38  105578  0.048840 -0.289464  -0.006818  0.024487 -0.612073  -1.659524   \n",
       "39  105579  0.064709 -0.255892  -0.006818  0.056506 -0.734174  -1.659524   \n",
       "40  105580  0.105437 -0.163076  -0.006818  0.145030 -0.847520  -1.659524   \n",
       "41  105581  0.104379 -0.311186  -0.006818 -0.003766 -0.873242  -1.659524   \n",
       "42  105582  0.135587 -0.198622  -0.006818  0.111128 -0.789069  -1.659524   \n",
       "43  105583  0.139818 -0.299338  -0.006818  0.015069 -0.654897  -1.659524   \n",
       "44  105584  0.135058 -0.297363  -0.006818 -0.016950 -0.557264  -1.659524   \n",
       "45  105585  0.137174 -0.307237  -0.006818  0.007535 -0.548603  -1.659524   \n",
       "46  105586  0.126595 -0.275640  -0.006818 -0.037669 -0.625789  -1.659524   \n",
       "47  105587  0.127124 -0.313161  -0.006818  0.001885 -0.734575  -1.659524   \n",
       "48  105588  0.130826 -0.301312  -0.006818  0.013186 -0.797098  -1.659524   \n",
       "49  105589  0.130298 -0.313161  -0.006818 -0.001882 -0.760468  -1.659524   \n",
       "50  105590  0.123421 -0.289464  -0.006818 -0.024484 -0.651261  -1.659524   \n",
       "51  105591  0.124479 -0.311186  -0.006818  0.003768 -0.553929  -1.659524   \n",
       "52  105592  0.622215  1.543158  -0.006818  1.772370 -0.539035  -1.659524   \n",
       "53  105593  2.106429  5.226174  -0.006818  5.285089 -0.629372  -1.659524   \n",
       "54  105594  2.126000 -0.242068  -0.006818  0.069691 -0.799598  -1.659524   \n",
       "55  105595  2.131818 -0.293413  -0.006818  0.020720 -0.975499  -1.659524   \n",
       "56  105596  2.127587 -0.299338  -0.006818 -0.015067 -1.054998  -1.659524   \n",
       "57  105597  2.123355 -0.299338  -0.006818 -0.015067 -0.979371  -1.659524   \n",
       "58  105598  2.112247 -0.273665  -0.006818 -0.039552 -0.807754  -1.659524   \n",
       "59  105599  2.114892 -0.305262  -0.006818  0.009419 -0.658369  -1.659524   \n",
       "60  105600  2.100082 -0.259841  -0.006818 -0.052737 -0.607730  -1.659524   \n",
       "61  105601  2.093734 -0.291438  -0.006818 -0.022601 -0.665103  -1.659524   \n",
       "62  105602  1.710780  1.114625  -0.006818 -1.363649 -0.794700  -1.659524   \n",
       "63  105603  1.444192  0.680167  -0.006818 -0.949280 -0.930708  -1.659524   \n",
       "64  105604  1.426208 -0.247993  -0.006818 -0.064038 -0.995331  -1.659524   \n",
       "65  105605  1.421448 -0.297363  -0.006818 -0.016950 -0.955258  -1.659524   \n",
       "66  105606  0.162034  4.386881  -0.006818 -4.484601 -0.856938  -1.659524   \n",
       "67  105607 -0.116190  0.723613  -0.006818 -0.990717 -0.779326  -1.659524   \n",
       "68  105608 -0.498087  1.110675  -0.006818 -1.359882 -0.772474  -1.659524   \n",
       "69  105609 -0.580602 -0.007066  -0.006818 -0.293824 -0.840591  -1.659524   \n",
       "70  105610 -0.551510 -0.206522  -0.006818  0.103594 -0.939862  -1.659524   \n",
       "71  105611 -0.553097 -0.309212  -0.006818 -0.005649 -1.006108  -1.659524   \n",
       "72  105612 -0.552039 -0.311186  -0.006818  0.003768 -0.993884  -1.659524   \n",
       "73  105613 -0.553097 -0.311186  -0.006818 -0.003766 -0.918583  -1.659524   \n",
       "74  105614 -0.611281 -0.097907  -0.006818 -0.207183 -0.843617  -1.659524   \n",
       "75  105615 -0.626620 -0.257867  -0.006818 -0.054620 -0.828263  -1.659524   \n",
       "76  105616 -0.598057 -0.208496  -0.006818  0.101710 -0.893496  -1.659524   \n",
       "77  105617 -0.622389 -0.224295  -0.006818 -0.086639 -1.017953  -1.659524   \n",
       "78  105618 -0.612339 -0.277615  -0.006818  0.035788 -1.137791  -1.659524   \n",
       "79  105619 -0.583776 -0.208496  -0.006818  0.101710 -1.173854  -1.659524   \n",
       "80  105620 -0.586949 -0.303287  -0.006818 -0.011300 -1.090888  -1.659524   \n",
       "81  105621 -0.595941 -0.281564  -0.006818 -0.032018 -0.939486  -1.659524   \n",
       "82  105622 -0.593826 -0.307237  -0.006818  0.007535 -0.808089  -1.659524   \n",
       "83  105623 -0.599644 -0.293413  -0.006818 -0.020717 -0.754436  -1.659524   \n",
       "84  105624 -0.596999 -0.305262  -0.006818  0.009419 -0.785236  -1.659524   \n",
       "85  105625 -0.600173 -0.303287  -0.006818 -0.011300 -0.873291  -1.659524   \n",
       "86  105626 -0.600173 -0.315136  -0.006818  0.000001 -0.944305  -1.659524   \n",
       "87  105627 -0.598057 -0.307237  -0.006818  0.007535 -0.917153  -1.659524   \n",
       "88  105628 -0.601760 -0.301312  -0.006818 -0.013183 -0.779301  -1.659524   \n",
       "89  105629 -0.602818 -0.311186  -0.006818 -0.003766 -0.601494  -1.515023   \n",
       "90  105630 -0.605991 -0.303287  -0.006818 -0.011300 -0.476548  -1.515023   \n",
       "91  105631 -0.603347 -0.305262  -0.006818  0.009419 -0.444758  -1.515023   \n",
       "92  105632 -0.605991 -0.305262  -0.006818 -0.009416 -0.499406  -1.515023   \n",
       "93  105633 -0.608107 -0.307237  -0.006818 -0.007533 -0.591366  -1.515023   \n",
       "94  105634 -0.609165 -0.311186  -0.006818 -0.003766 -0.658119  -1.515023   \n",
       "95  105635 -0.609165 -0.315136  -0.006818  0.000001 -0.664700  -1.515023   \n",
       "96  105636 -0.609165 -0.315136  -0.006818  0.000001 -0.621536  -1.515023   \n",
       "97  105637 -0.610223 -0.311186  -0.006818 -0.003766 -0.566331  -1.515023   \n",
       "98  105638 -0.609165 -0.311186  -0.006818  0.003768 -0.536985  -1.515023   \n",
       "99  105639 -0.603876 -0.295388  -0.006818  0.018836 -0.552604  -1.515023   \n",
       "\n",
       "       hurst       max  nonlinear       var  dummy_id  \n",
       "0   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "1   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "2   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "3   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "4   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "5   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "6   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "7   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "8   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "9   0.325913  1.050126  -0.542338  0.673127         1  \n",
       "10  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "11  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "12  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "13  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "14  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "15  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "16  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "17  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "18  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "19  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "20  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "21  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "22  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "23  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "24  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "25  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "26  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "27  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "28  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "29  0.325913  1.050126  -0.542338  0.673127         1  \n",
       "30  0.327174  1.050126  -0.541600  0.728126         1  \n",
       "31  0.328183  1.050126  -0.540772  0.782525         1  \n",
       "32  0.327108  1.050126  -0.540536  0.801349         1  \n",
       "33  0.325441  1.050126  -0.530947  0.800779         1  \n",
       "34  0.324612  1.050126  -0.528210  0.798270         1  \n",
       "35  0.323697  1.050126  -0.529938  0.793166         1  \n",
       "36  0.304998  1.050126  -0.426930  0.765110         1  \n",
       "37  0.301508  1.050126  -0.419911  0.745114         1  \n",
       "38  0.298972  1.050126  -0.422190  0.741137         1  \n",
       "39  0.299238  1.050126  -0.378153  0.739002         1  \n",
       "40  0.298662  1.050126  -0.321732  0.726937         1  \n",
       "41  0.296953  1.050126  -0.254854  0.707632         1  \n",
       "42  0.304879  1.050126  -0.403707  0.683860         1  \n",
       "43  0.313861  1.050126  -0.484818  0.698237         1  \n",
       "44  0.316627  1.050126  -0.461231  0.720795         1  \n",
       "45  0.318485  1.050126  -0.441474  0.736902         1  \n",
       "46  0.319069  1.050126  -0.444668  0.739870         1  \n",
       "47  0.319522  1.050126  -0.442053  0.742952         1  \n",
       "48  0.319934  1.050126  -0.443336  0.746444         1  \n",
       "49  0.320170  1.050126  -0.446212  0.747339         1  \n",
       "50  0.320295  1.050126  -0.445838  0.747163         1  \n",
       "51  0.320382  1.050126  -0.445543  0.747723         1  \n",
       "52  0.320401  1.050126  -0.445584  0.747958         1  \n",
       "53  0.320368  1.050126  -0.445073  0.748578         1  \n",
       "54  0.320266  1.050126  -0.445211  0.749196         1  \n",
       "55  0.320066  1.050126  -0.446560  0.749963         1  \n",
       "56  0.319717  1.050126  -0.446789  0.749033         1  \n",
       "57  0.319264  1.050126  -0.447078  0.747831         1  \n",
       "58  0.319330  1.050126  -0.436223  0.746745         1  \n",
       "59  0.322090  1.050126  -0.422531  0.752675         1  \n",
       "60  0.321828  1.050126  -0.420823  0.774955         1  \n",
       "61  0.322164  1.050126  -0.419278  0.796737         1  \n",
       "62  0.322485  1.050126  -0.416510  0.818271         1  \n",
       "63  0.322791  1.050126  -0.413729  0.839450         1  \n",
       "64  0.323079  1.050126  -0.409950  0.860231         1  \n",
       "65  0.323346  1.050126  -0.405928  0.880492         1  \n",
       "66  0.323595  1.050126  -0.402111  0.900252         1  \n",
       "67  0.323833  1.050126  -0.398672  0.919617         1  \n",
       "68  0.324060  1.050126  -0.395011  0.938524         1  \n",
       "69  0.324277  1.050126  -0.392546  0.956747         1  \n",
       "70  0.324514  1.050126  -0.393460  0.974551         1  \n",
       "71  0.324730  1.050126  -0.387393  0.992301         1  \n",
       "72  0.324966  1.050126  -0.386559  1.009542         1  \n",
       "73  0.325175  1.050126  -0.380914  1.026159         1  \n",
       "74  0.325368  1.050126  -0.372338  1.042196         1  \n",
       "75  0.325401  1.050126  -0.363473  1.057591         1  \n",
       "76  0.315991  1.050126  -0.425083  1.060575         1  \n",
       "77  0.304252  1.050126  -0.267211  1.116698         1  \n",
       "78  0.307336  1.050126  -0.236609  1.171829         1  \n",
       "79  0.309956  1.050126  -0.188556  1.225095         1  \n",
       "80  0.311930  1.050126  -0.115798  1.276644         1  \n",
       "81  0.313748  1.050126  -0.009663  1.326070         1  \n",
       "82  0.321475  1.050126  -0.150362  1.373860         1  \n",
       "83  0.338758  1.050126  -0.476507  1.414337         1  \n",
       "84  0.337693  1.050126  -0.491593  1.339442         1  \n",
       "85  0.336793  1.050126  -0.503546  1.261359         1  \n",
       "86  0.336293  1.046372  -0.512004  1.181862         1  \n",
       "87  0.334494  1.042618  -0.516407  1.101911         1  \n",
       "88  0.331873  1.035110  -0.514610  0.997109         1  \n",
       "89  0.316616  1.035110  -0.546609  0.866342         1  \n",
       "90  0.310773  1.021971  -0.560972  0.732682         1  \n",
       "91  0.303467  1.016340  -0.525110  0.606670         1  \n",
       "92  0.300038  0.676608  -0.542625  0.473411         1  \n",
       "93  0.296517  0.440109  -0.544921  0.382290         1  \n",
       "94  0.291377  0.424155  -0.547757  0.315007         1  \n",
       "95  0.284638  0.419932  -0.548959  0.244786         1  \n",
       "96  0.306666  0.388493  -0.476350  0.170412         1  \n",
       "97  0.308951  0.388493  -0.517800  0.173902         1  \n",
       "98  0.311812  0.388493  -0.482892  0.180724         1  \n",
       "99  0.312346  0.388493  -0.464523  0.184404         1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll_time_series method creates sub windows of the time series. It rolls the (sorted) data frames for each \n",
    "# kind and each id separately in the “time” domain (which is represented by the sort order of the sort column given\n",
    "# by column_sort).\n",
    "# For example when applying the roll_time_series to a data [a,b,c,d,e,f,g] with time_shift = 3, the result will be \n",
    "# [a,b,c, b,c,d, c,d,e, d,e,f, e,f,g]. The input of the CNN model will have a shape of [30,10] (30 timestep, 10\n",
    "# variables) and its prediction will be the appliances status at time_step 30.\n",
    "# Notice: run these 3 lines of code will take hours\n",
    "\n",
    "\n",
    "x_test_rolled = roll_time_series(x_test, column_id=\"dummy_id\", column_sort=\"id\",\n",
    "                            max_timeshift = 29, min_timeshift = 29, n_jobs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d22fcd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 80/80 [1:02:39<00:00, 47.00s/it] \n"
     ]
    }
   ],
   "source": [
    "x_test_rolled.to_hdf('../x_test_rolled.h5','x_test_rolled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5efc87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/starlet/anaconda3/envs/spconv12cuda11/lib/python3.8/site-packages/pandas/core/generic.py:2606: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id'], dtype='object')]\n",
      "\n",
      "  pytables.to_hdf(\n"
     ]
    }
   ],
   "source": [
    "x_test_rolled.to_hdf('../x_test_rolled.h5','x_test_rolled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cc20ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417720.0\n",
      "417720\n"
     ]
    }
   ],
   "source": [
    "# check if there're any shape differences between rolled data and original data\n",
    "print(len(x_test_rolled)/30)\n",
    "print(len(x_test)-29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c3ce87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(417720, 30, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the ID and dummy_id columns\n",
    "x_test_rolled.drop(['id','dummy_id'], axis=1, inplace=True)\n",
    "# reshape the rolled x_train to a new shape of [105540,30,10]\n",
    "x_test_rolled = np.reshape(x_test_rolled.to_numpy(dtype=np.float64), [-1 , 30, int(x_test_rolled.shape[1])])\n",
    "x_test_rolled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff43a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html\n",
    "# Compute the one-dimensional discrete Fourier Transform for real input.\n",
    "# This function computes the one-dimensional n-point discrete Fourier Transform (DFT)\n",
    "# of a real-valued array by means of an efficient algorithm called the Fast Fourier Transform (FFT).\n",
    "x_test_fft = np.copy(x_test_rolled)\n",
    "x_test_fft = np.apply_along_axis(absfft, 1, x_test_fft)\n",
    "x_test_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6b812f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417720, 30, 10)\n",
      "(417720, 16, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_test_rolled.shape)\n",
    "print(x_test_fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2836682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the shape x_test and x_test_fft to [observations, features, time_steps]\n",
    "x_test = x_test_rolled.transpose(0,2,1)\n",
    "x_test_fft = x_test_fft.transpose(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63d8cdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417720, 10, 30)\n",
      "(417720, 10, 16)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(x_test_fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1885d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_test = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14664932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the testing dataset\n",
    "test_ds = TensorDataset(torch.tensor(x_test).float(), torch.tensor(x_test_fft).float())\n",
    "# Get the testing dataloader\n",
    "test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "test_preds = [] # to store prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab58059d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a new classifier model\n",
    "model = Classifier(10, 10, 20)\n",
    "# load the state dict of the previously trained models\n",
    "model.load_state_dict(torch.load(\"./bestf1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee48a8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (raw): Sequential(\n",
       "    (0): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(10, 10, kernel_size=(8,), stride=(2,), padding=(4,), groups=10)\n",
       "          (pointwise): Conv1d(10, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(32, 32, kernel_size=(8,), stride=(2,), padding=(3,), groups=32)\n",
       "          (pointwise): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(64, 64, kernel_size=(8,), stride=(2,), padding=(5,), groups=64)\n",
       "          (pointwise): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(128, 128, kernel_size=(8,), stride=(2,), padding=(2,), groups=128)\n",
       "          (pointwise): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Flatten()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (fft): Sequential(\n",
       "    (0): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(10, 10, kernel_size=(8,), stride=(2,), padding=(3,), groups=10)\n",
       "          (pointwise): Conv1d(10, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(32, 32, kernel_size=(8,), stride=(2,), padding=(5,), groups=32)\n",
       "          (pointwise): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(64, 64, kernel_size=(8,), stride=(2,), padding=(4,), groups=64)\n",
       "          (pointwise): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(128, 128, kernel_size=(8,), stride=(2,), padding=(5,), groups=128)\n",
       "          (pointwise): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): SepConv1d(\n",
       "      (layers): Sequential(\n",
       "        (0): _SepConv1d(\n",
       "          (depthwise): Conv1d(128, 128, kernel_size=(8,), stride=(2,), padding=(3,), groups=128)\n",
       "          (pointwise): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Flatten()\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Dropout(p=0.5, inplace=False)\n",
       "    (10): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (11): ReLU()\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the model mode to evaluate and send to the device\n",
    "model.eval()\n",
    "model.to(device_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69638696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/417720 [00:00<?, ?it/s]/home/starlet/anaconda3/envs/spconv12cuda11/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "100%|██████████| 417720/417720 [14:54<00:00, 466.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform predicting on testing date\n",
    "for test_batch in tqdm(test_dl): # Iterate through every batch of testing dataloader\n",
    "    x_raw, x_fft = [t.to(device_test) for t in test_batch] # Send the data to device\n",
    "    out = model(x_raw, x_fft) # Get the output\n",
    "    preds = F.log_softmax(out, dim=1).argmax(dim=1) # Get the final prediction\n",
    "    # decode its value for submisson compatability\n",
    "    decoded_preds = le_y.inverse_transform([preds.detach().cpu().clone().numpy()]) \n",
    "    # Save the prediction\n",
    "    test_preds.append([i for i in decoded_preds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1cd86bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ac</th>\n",
       "      <th>ev</th>\n",
       "      <th>oven</th>\n",
       "      <th>wash</th>\n",
       "      <th>dryer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ac ev oven wash dryer\n",
       "id                      \n",
       "1   1  0    0    0     0\n",
       "2   0  0    0    0     0\n",
       "3   0  0    0    0     0\n",
       "4   0  0    0    0     0\n",
       "5   0  0    0    0     0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the prediction list to dataframe\n",
    "predictions = pd.DataFrame(np.array(test_preds)).reset_index()\n",
    "predictions.columns = ['id', 'ac', 'ev', 'oven', 'wash', 'dryer'] # Add column names\n",
    "predictions['id'] = predictions['id'] + 1 # For submission compatability\n",
    "predictions.set_index('id',inplace=True) # set ID column as index\n",
    "predictions.head() #have a look to check its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e549282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ac</th>\n",
       "      <th>ev</th>\n",
       "      <th>oven</th>\n",
       "      <th>wash</th>\n",
       "      <th>dryer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ac ev oven wash dryer\n",
       "id                      \n",
       "1   1  0    0    0     0\n",
       "2   0  0    0    0     0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many classes are predicted\n",
    "predictions[['ac', 'ev', 'oven', 'wash', 'dryer']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "caeb797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to .csv file for submission\n",
    "predictions.to_csv('../CNN_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733c3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8eb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97eefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b8c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b01a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7ef93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84de3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb5551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca2290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01d85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad59b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec238f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bed015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(TEST_DATA_DIR)\n",
    "data.rename(columns={\"Unnamed: 0\":\"id\"}, inplace= True)\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cca961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Have a look at the datatypes, seeing that the variable \"dayofweek\" has data type \"object\" which is unfarvorable\n",
    "# when training deep learning models, will need to encode and change it to float datatype\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into x_train and y_train\n",
    "x_train = data.loc[:,data.columns.difference(['ac', 'ev', 'oven', 'wash', 'dryer'])]\n",
    "y_train = data[['ac', 'ev', 'oven', 'wash', 'dryer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd47f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder() # Create a label encoder for variable \"dayofweeek\"\n",
    "le.fit(x_train['dayofweek']) # Fit the values\n",
    "x_train['dayofweek'] = le.transform(x_train['dayofweek']) # Perform transformation\n",
    "le.classes_ # have a look at the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa8b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dummy ID columns to make it compatible with tsfresh library\n",
    "x_train['dummy_id'] = 1 \n",
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scale_list = ['absdif', 'dayofweek', 'dif', 'entropy', 'hourofday', 'hurst',\n",
    "       'load', 'max', 'nonlinear', 'var']\n",
    "scaler = preprocessing.StandardScaler().fit(x_train[scale_list]) # Fit the data values\n",
    "x_train[scale_list] = scaler.transform(x_train[scale_list]) # Perform transforming\n",
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a72fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since the design choice is to use 30 minutes at each time-step, therefore each of the first 29 observations \n",
    "# will not have enough 29 observations before it in order to make a series of 30 observations.\n",
    "# Thus I will add 29 copies of the first observation to the top of the dataframe.\n",
    "extra_len = pd.DataFrame(x_train.iloc[0:1,:].values.repeat(29, axis = 0), columns=x_train.columns)\n",
    "\n",
    "extra_len['id'] = list(range(105541-29,105541)) # Adjust the indexes\n",
    "x_train = pd.concat([extra_len, x_train]) # Concatenate the x_train and the extra data\n",
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b127ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the values of the 5 appliances into a string, for example 0,1,0,0,1 will be 01001\n",
    "y_train['transformed'] = y_train.apply(lambda x: ''.join(x.astype(str)),axis = 1)\n",
    "# create a label encoder for y_train\n",
    "le_y = preprocessing.LabelEncoder()\n",
    "# fit and transform the values of the concatenated values \n",
    "y_train['encoded'] = le_y.fit_transform(y_train['transformed'])\n",
    "# Extract the encoded values to use them as training target \n",
    "y_train_transformed = y_train['encoded']\n",
    "y_train_transformed = np.array(y_train_transformed)\n",
    "y_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad66724",
   "metadata": {},
   "source": [
    "### Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae95946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll_time_series method creates sub windows of the time series. It rolls the (sorted) data frames for each \n",
    "# kind and each id separately in the “time” domain (which is represented by the sort order of the sort column given\n",
    "# by column_sort).\n",
    "# For example when applying the roll_time_series to a data [a,b,c,d,e,f,g] with time_shift = 3, the result will be \n",
    "# [a,b,c, b,c,d, c,d,e, d,e,f, e,f,g]. The input of the CNN model will have a shape of [30,10] (30 timestep, 10\n",
    "# variables) and its prediction will be the appliances status at time_step 30.\n",
    "# Notice: run these 3 lines of code will take hours\n",
    "\n",
    "x_train = x_train[['id', 'load', 'absdif', 'dayofweek', 'dif', 'entropy', 'hourofday', 'hurst', \n",
    "        'max', 'nonlinear', 'var']]\n",
    "x_train_rolled = roll_time_series(x_train, column_id=\"dummy_id\", column_sort=\"id\",\n",
    "                            max_timeshift = 29, min_timeshift = 29)\n",
    "x_train_rolled.to_hdf('../x_train_rolled.h5','x_train_rolled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524712c",
   "metadata": {},
   "source": [
    "### Load rolled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30187e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rolled = pd.read_hdf(\"../x_train_rolled.h5\",\"x_train_rolled\")\n",
    "x_train_rolled.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca452fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The training data now has 12531600 observation, which is 30 times bigger than the original dataset, this is \n",
    "# because of the rolling method mentioned above.\n",
    "print(len(x_train_rolled)/30) \n",
    "print(len(x_train)-29)\n",
    "del x_train\n",
    "# Let's have a look to see if the amount of data is the same as the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c833aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the data after being encoded, we only use the last columns as y_train.\n",
    "y_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c8f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next, let's go back to the rolled x_train data\n",
    "x_train_rolled.tail(60)\n",
    "# It can be seen that for each 30 observations, they all have the same ID, which mean an observation K and\n",
    "# K-1, K-2, ... K-29 observation will be used as input for the model to predict the y value of observation K.\n",
    "# There are 12531600 rows in this dataset, which is 30 times bigger than the original one because that each \n",
    "# row and its 29 previous rows will be used with 10 columns to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rolled.drop('id', axis=1, inplace=True) # drop the ID column\n",
    "# reshape the rolled x_train to a new shape of [417720,30,10]\n",
    "# 417720 is the number of observation, 30 is the number of timestep of each input, 10 is number of features\n",
    "x_train_rolled = np.reshape(x_train_rolled.to_numpy(dtype=np.float64), [-1 , 30, int(x_train_rolled.shape[1])])\n",
    "x_train_rolled.shape #(417720, 30, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc02b07",
   "metadata": {},
   "source": [
    "### Extract the Fast Fourier Transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe3e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html\n",
    "# Compute the one-dimensional discrete Fourier Transform for real input.\n",
    "# This function computes the one-dimensional n-point discrete Fourier Transform (DFT)\n",
    "# of a real-valued array by means of an efficient algorithm called the Fast Fourier Transform (FFT).\n",
    "\n",
    "def absfft(x):\n",
    "    return np.abs(np.fft.rfft(x))\n",
    "x_train_fft = np.copy(x_train_rolled)\n",
    "x_train_fft = np.apply_along_axis(absfft, 1, x_train_fft)\n",
    "x_train_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b758b9b",
   "metadata": {},
   "source": [
    "There are 16 (n/2+1) Fast Fourier Transform values computed from the 30 (n) rows, We will use these FFT as \n",
    "another input for our CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_rolled.shape)\n",
    "print(x_train_fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d870c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the shape x_train_rolled and x_train_fft to [observations, features, time_steps]\n",
    "x_train = x_train_rolled.transpose(0,2,1)\n",
    "x_train_fft = x_train_fft.transpose(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b65778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train_fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa05f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the distribution of target values, the data is heavily imbalanced\n",
    "plt.hist(y_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52746e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, used in building training and validating datasets\n",
    "def build_datasets(data, target, train_size, valid_pct = 0.2, seed = None):\n",
    "    x, x_fft = data # split the data in to raw and FFT data\n",
    "    idx = np.arange(train_size) # Create a list of indexes\n",
    "#     train_idx, val_idx = train_test_split(idx, test_size = valid_pct, random_state = seed)\n",
    "    # Start spliting the data into training and validating parts using the validate percentage (valid_pct) value\n",
    "    train_idx, val_idx = idx[round(train_size * valid_pct):], idx[:round(train_size * valid_pct)]\n",
    "    # Build the train data, which include raw x, FFT x, and target y \n",
    "    train_ds = TensorDataset(torch.tensor(x[:train_size][train_idx]).float(),\n",
    "                            torch.tensor(x_fft[:train_size][train_idx]).float(),\n",
    "                            torch.tensor(target[:train_size][train_idx]).long())\n",
    "    print(\"There are\",len(set(target[:train_size][train_idx])),\"class in training data\")\n",
    "    # Build the validating data, which include raw x, FFT x, and target y \n",
    "    val_ds = TensorDataset(torch.tensor(x[:train_size][val_idx]).float(),\n",
    "                            torch.tensor(x_fft[:train_size][val_idx]).float(),\n",
    "                            torch.tensor(target[:train_size][val_idx]).long())\n",
    "    return train_ds, val_ds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, \n",
    "def build_loaders(data, batch_size = 128, jobs = 8):\n",
    "    train_ds, valid_ds = data\n",
    "    # Build a train dataloader\n",
    "    train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = False, num_workers = jobs)\n",
    "    # Build a test dataloader\n",
    "    valid_dl = DataLoader(valid_ds, batch_size = batch_size, shuffle = False, num_workers = jobs)\n",
    "    return train_dl, valid_dl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/purplejester/pytorch-deep-time-series-classification\n",
    "class _SepConv1d(nn.Module):\n",
    "    \"\"\"A simple separable convolution implementation.\n",
    "    \n",
    "    The separable convlution is a method to reduce number of the parameters \n",
    "    in the deep learning network for slight decrease in predictions quality.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, no, kernel, stride, pad):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(ni, ni, kernel, stride, padding=pad, groups=ni)\n",
    "        self.pointwise = nn.Conv1d(ni, no, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise(self.depthwise(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6135c",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/purplejester/pytorch-deep-time-series-classification  \n",
    "\n",
    "To design a classifier, we'll first develop a few of helper classes. There are no specialised SeparableConv layers in the torch framework. However, we can simply reproduce them using the following class. (This was taken from a topic on the PyTorch forum.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fbfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SepConv1d(nn.Module):\n",
    "    \"\"\"Implementes a 1-d convolution with 'batteries included'.\n",
    "    \n",
    "    The module adds (optionally) activation function and dropout \n",
    "    layers right after a separable convolution layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, no, kernel, stride, pad, \n",
    "                 drop=None, bn=False,\n",
    "                 activ=lambda: nn.ReLU()):\n",
    "    \n",
    "        super().__init__()\n",
    "        # Check the drop out rate\n",
    "        assert drop is None or (0.0 < drop < 1.0)\n",
    "        # Build a separable convolution layer, using channel_in, channel_out, kernel size, stride, and padding size \n",
    "        layers = [_SepConv1d(ni, no, kernel, stride, pad)]\n",
    "        # Add an activation function\n",
    "        if activ:\n",
    "            layers.append(activ())\n",
    "        # Apply batch normalization if required\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm1d(no))\n",
    "        # Apply drop out rate to prevent the model from overfitting\n",
    "        if drop is not None:\n",
    "            layers.append(nn.Dropout(drop))\n",
    "        # chain all of the layers into one object\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    # Define forward function\n",
    "    def forward(self, x): \n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, used to build flatten layers\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "    # Define forward function, which flatten the input into 1 dimension, or 2 dimension which is [batchsize, -1]\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to print the shape of a layer's output, this function is useful when building and debugging \n",
    "# models\n",
    "class PrintSize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintSize, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building, which includes 2 branches, one brach for raw input with 30 time steps, one branch for \n",
    "# FFT input with 30/2 + 1 = 16 timesteps.\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, raw_ni, fft_ni, no, drop=.5):\n",
    "        super().__init__()\n",
    "        #PKS [[4,8,2],[1,3,1],[3,8,2],[1,3,1],[5,8,2],[1,3,1],[2,8,2]]\n",
    "        self.raw = nn.Sequential( \n",
    "            #         (in ,out ,kernel, stride, pad)\n",
    "            SepConv1d(raw_ni,  32, 8, 2, 4, drop=drop),\n",
    "            SepConv1d(    32,  64, 8, 2, 3, drop=drop),\n",
    "            SepConv1d(    64, 128, 8, 2, 5, drop=drop),\n",
    "            SepConv1d(   128, 256, 8, 2, 2, drop=drop),\n",
    "            Flatten(),\n",
    "#             PrintSize(),\n",
    "            nn.Dropout(drop), nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Dropout(drop), nn.Linear(256, 64), nn.ReLU())\n",
    "        #PKS [[3,8,2],[1,3,1],[5,8,2],[1,3,1],[4,8,2],[1,3,1],[3,8,2]]\n",
    "        self.fft = nn.Sequential(\n",
    "            SepConv1d(fft_ni,  32, 8, 2, 3, drop=drop),\n",
    "            SepConv1d(    32,  64, 8, 2, 5, drop=drop),\n",
    "            SepConv1d(    64, 128, 8, 2, 4, drop=drop),\n",
    "            SepConv1d(   128, 128, 8, 2, 5, drop=drop),\n",
    "            SepConv1d(   128, 256, 8, 2, 3, drop=drop),\n",
    "            Flatten(),\n",
    "#             PrintSize(),\n",
    "            nn.Dropout(drop), nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Dropout(drop), nn.Linear(256, 64), nn.ReLU())\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.ReLU(inplace=True), nn.Linear(64, no))\n",
    "        self.init_weights(nn.init.kaiming_normal_)\n",
    "        \n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        def init(m): \n",
    "            for child in m.children():\n",
    "                if isinstance(child, nn.Conv1d):\n",
    "                    # Fills the input Tensor with values according to the method described in Delving deep into \n",
    "                    # rectifiers: Surpassing human-level performance on \n",
    "                    # ImageNet classification - He, K. et al. (2015), using a normal distribution\n",
    "                    init_fn(child.weights)\n",
    "        init(self)\n",
    "        \n",
    "    # Define a forward function for the model\n",
    "    def forward(self, t_raw, t_fft):\n",
    "        # raw x branch\n",
    "        raw_out = self.raw(t_raw)\n",
    "        # fft x branch\n",
    "        fft_out = self.fft(t_fft)\n",
    "        # concat 2 branches into one\n",
    "        t_in = torch.cat([raw_out, fft_out], dim=1)\n",
    "        # push it through fully connected layers to get the output\n",
    "        out = self.out(t_in)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5eaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, raw_ni, fft_ni, no, drop=.5):\n",
    "#         super().__init__()\n",
    "#         #PKS [[4,8,2],[3,8,2],[5,8,2],[2,8,2]]\n",
    "#         self.raw = nn.Sequential( #kernel, stride, pad\n",
    "#             SepConv1d(raw_ni,  32, 8, 2, 4, drop=drop),\n",
    "#             SepConv1d(    32,  64, 8, 2, 3, drop=drop),\n",
    "#             SepConv1d(    64, 128, 8, 2, 5, drop=drop),\n",
    "#             SepConv1d(   128, 256, 8, 2, 2, drop=drop),\n",
    "#             Flatten(),\n",
    "# #             PrintSize(),\n",
    "#             nn.Dropout(drop), nn.Linear(512, 256), nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop), nn.Linear( 256, 64), nn.ReLU(inplace=True))\n",
    "#         #PKS [3,8,2],[5,8,2],[4,8,2],[5,8,2],[3,8,2]\n",
    "#         self.fft = nn.Sequential(\n",
    "#             SepConv1d(fft_ni,  32, 8, 2, 3, drop=drop),\n",
    "#             SepConv1d(    32,  64, 8, 2, 5, drop=drop),\n",
    "#             SepConv1d(    64, 128, 8, 2, 4, drop=drop),\n",
    "#             SepConv1d(   128, 128, 8, 2, 5, drop=drop),\n",
    "#             SepConv1d(   128, 256, 8, 2, 3, drop=drop),\n",
    "#             Flatten(),\n",
    "# #             PrintSize(),\n",
    "#             nn.Dropout(drop), nn.Linear(512, 256), nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop), nn.Linear( 256, 64), nn.ReLU(inplace=True))\n",
    "        \n",
    "#         self.out = nn.Sequential(\n",
    "#             nn.Linear(128, 64), nn.ReLU(inplace=True), nn.Linear(64, no))\n",
    "        \n",
    "#     def forward(self, t_raw, t_fft):\n",
    "#         raw_out = self.raw(t_raw)\n",
    "#         fft_out = self.fft(t_fft)\n",
    "#         t_in = torch.cat([raw_out, fft_out], dim=1)\n",
    "#         out = self.out(t_in)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gokulprasadthekkel/pytorch-multi-class-focal-loss/blob/master/focal_loss.py\n",
    "# Helper function, implementation of focal loss in order to solve class imbalance problem\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acaab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "# Since we use the provided data for only training and validating, thus no testing data needed. Thus we set the \n",
    "# train size to the full length of x_train.\n",
    "train_size = len(x_train)\n",
    "# build data set with validation percentage = 10%\n",
    "datasets = build_datasets((x_train, x_train_fft), y_train_transformed,\n",
    "                          train_size = train_size, valid_pct = 0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9838a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of different classes in the data\n",
    "cls_count = dict(collections.Counter(y_train_transformed)) # count the number of appearance per class\n",
    "cls_weights = np.array([i[1] for i in sorted(cls_count.items())]) # sort the classes by descending order of class \n",
    "                                                                  # appearance count\n",
    "cls_weights = torch.Tensor([cls_weights/np.sum(cls_weights)]) # calculate the proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for training, this model is trained on a RTX 3080, thus the device will be \"cuda\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c87b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the input number of features of the x_raw branch of the model, which is 30\n",
    "raw_feat = x_train.shape[1]\n",
    "# Get the input number of features of the x_fft branch of the model, which is 16\n",
    "fft_feat = x_train_fft.shape[1]\n",
    "# Get the train and validate dataloaders\n",
    "trn_dl, val_dl = build_loaders(datasets, batch_size=512)\n",
    "\n",
    "lr = 0.0002 # Set learning rate\n",
    "n_epochs = 400 # Set maximum epoch\n",
    "iterations_per_epoch = len(trn_dl) # Iteration per epoch\n",
    "num_classes = 20 # Number of classes\n",
    "best_acc = 0 # Set best accuracy for later early stopping purpose\n",
    "best_f1 = 0 # Set best F1_score for later early stopping purpose\n",
    "patience, trials = 50, 0 # Set patience and a counter for later early stopping purpose\n",
    "base = 1 # use to efficiently print the loss and accuracy while training \n",
    "step = 2 # use to efficiently print the loss and accuracy while training, only print out the accuracy at\n",
    "         # step 2,4,8,16,32,...\n",
    "loss_history = [] # For loss storing\n",
    "acc_history = [] # For accuracy storing\n",
    "gstep = -1 # Global step - used to initiate an One Cycle Learning Rate Scheduler. gstep = -1 means entirely new model\n",
    "model = Classifier(raw_feat, fft_feat, num_classes,drop = .5).to(device) # Create the CNN model \n",
    "# criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "cls_weights = cls_weights.to(device) # Send the class weight to cuda device\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum') # Create loss function\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay= 0.0001) # Initiate Adam optimizer\n",
    "# Create a one cycle learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, 0.004, steps_per_epoch = iterations_per_epoch,\n",
    "                                                   epochs = n_epochs, pct_start = 0.4,base_momentum = 0.85, \n",
    "                                                   max_momentum=0.95, div_factor = 10.0, last_epoch = gstep)\n",
    "print('Start model training')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Set model to traning mode\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # Iterate through every batch of the train data loader\n",
    "    for i, batch in enumerate(trn_dl):\n",
    "        # Send the input to CUDA device\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        opt.zero_grad()\n",
    "        out = model(x_raw, x_fft) # Get the output from the model\n",
    "        loss = criterion(out, y_batch) # Calculate losses\n",
    "        epoch_loss += loss.item() # Record the loss\n",
    "        loss.backward() # Back-propagation\n",
    "        opt.step() # Call step() method to update the parameters\n",
    "        lr_scheduler.step() # Call step() method to update the parameters\n",
    "    # average the epoch's loss\n",
    "    epoch_loss /= train_size\n",
    "    # Record the loss history\n",
    "    loss_history.append(epoch_loss)\n",
    "    # Start validating\n",
    "    model.eval()\n",
    "    correct, total, epoch_f1 = 0, 0, []\n",
    "    for batch in val_dl: # Iterate through every batch of the validating data\n",
    "        # Send the data to CUDA device\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        out = model(x_raw, x_fft) # Get the output from the model\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1) # Generate final prediction\n",
    "        # Calculate F1_score of the batch\n",
    "        batch_f1 = f1_score(y_batch.detach().cpu().clone().numpy(), preds.detach().cpu().clone().numpy(), \n",
    "                           average=\"macro\")\n",
    "        epoch_f1.append(batch_f1) # Record the error\n",
    "        total += y_batch.size(0) \n",
    "        correct += (preds == y_batch).sum().item() # calculate the accuracy\n",
    "    # average the F1_score of all batches to get the validation F1_score\n",
    "    cur_f1 = np.mean(epoch_f1)\n",
    "    epoch_f1 = []\n",
    "    # Get the validating accuracy   \n",
    "    acc = correct / total\n",
    "    acc_history.append(acc)\n",
    "\n",
    "    if epoch % base == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n",
    "        base *= step\n",
    "    # Keep track of the improvement of the model in term of accuracy for early stopping\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        # save the model with best accuracy\n",
    "        torch.save(model.state_dict(), 'bestaccu.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "    # Keep track of the improvement of the model in term of F1_score for early stopping\n",
    "    if cur_f1 > best_f1:\n",
    "        trials = 0\n",
    "        best_f1 = cur_f1\n",
    "        # save the model with best F1_score\n",
    "        torch.save(model.state_dict(), 'bestf1.pth')\n",
    "        print(f'Epoch {epoch} best F1 saved with F1: {best_f1:2.2%}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break\n",
    "            \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0404f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'latest.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9508eb1",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fa5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete redundant variables (optional)\n",
    "del x_train,x_train_fft,x_train_rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a new classifier model\n",
    "model = Classifier(10, 10, 20)\n",
    "# load the state dict of the previously trained models\n",
    "model.load_state_dict(torch.load(\"./bestf1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01587e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change its mode to evaluating\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing data\n",
    "x_test = pd.read_csv(\"../FIT5149_A2_data/test_data_nolabels.csv\")\n",
    "x_test.rename(columns={'Unnamed: 0': 'id'}, inplace= 1) # Rename \n",
    "x_test['id'] = x_test['id'] + 28 # Increase the ID column by 28 for later rolling\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bf79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the list of variables that will be scaled\n",
    "scale_list = ['absdif', 'dayofweek', 'dif', 'entropy', 'hourofday', 'hurst', 'load', 'max', 'nonlinear', 'var']\n",
    "# Use the label encoder of training data to encode the testing one\n",
    "x_test['dayofweek'] = le.transform(x_test['dayofweek'])\n",
    "x_test[scale_list] = scaler.transform(x_test[scale_list]) # Perform scaling\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e06c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the design choice is to use 30 minutes at each time-step, therefore each of the first 29 observations \n",
    "# will not have enough 29 observations before it in order to create a series of 30 observations.\n",
    "# Thus I will add 29 copies of the first observation to the top of the testing dataframe.\n",
    "extra_len = pd.DataFrame(x_test.iloc[0:1,:].values.repeat(29, axis = 0), columns=x_test.columns)\n",
    "extra_len['id'] = list(range(0,29))\n",
    "\n",
    "# concatenate the testing and the additional data\n",
    "x_test = pd.concat([extra_len, x_test])\n",
    "x_test['dummy_id'] = 1 # Create a dummy ID column for rolling\n",
    "x_test.reset_index(drop= True, inplace=True) # Reset index\n",
    "x_test.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll_time_series method creates sub windows of the time series. It rolls the (sorted) data frames for each \n",
    "# kind and each id separately in the “time” domain (which is represented by the sort order of the sort column given\n",
    "# by column_sort).\n",
    "# For example when applying the roll_time_series to a data [a,b,c,d,e,f,g] with time_shift = 3, the result will be \n",
    "# [a,b,c, b,c,d, c,d,e, d,e,f, e,f,g]. The input of the CNN model will have a shape of [30,10] (30 timestep, 10\n",
    "# variables) and its prediction will be the appliances status at time_step 30.\n",
    "# Notice: run these 3 lines of code will take hours\n",
    "\n",
    "\n",
    "# x_test_rolled = roll_time_series(x_test, column_id=\"dummy_id\", column_sort=\"id\",\n",
    "#                             max_timeshift = 29, min_timeshift = 29)\n",
    "# np.save(\"../np_x_test_rolled.npy\",np.array(x_test_rolled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_rolled = np.load('../np_x_test_rolled.npy', allow_pickle= True)\n",
    "x_test_rolled = pd.DataFrame(x_test_rolled, columns=x_test.columns)\n",
    "x_test_rolled = x_test_rolled[['id', 'load', 'absdif', 'dayofweek', 'dif', 'entropy', 'hourofday', 'hurst', \n",
    "        'max', 'nonlinear', 'var','dummy_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09729464",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_rolled.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there're any shape differences between rolled data and original data\n",
    "print(len(x_test_rolled)/30)\n",
    "print(len(x_test)-29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773cc517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the ID and dummy_id columns\n",
    "x_test_rolled.drop(['id','dummy_id'], axis=1, inplace=True)\n",
    "# reshape the rolled x_train to a new shape of [105540,30,10]\n",
    "x_test_rolled = np.reshape(x_test_rolled.to_numpy(dtype=np.float64), [-1 , 30, int(x_test_rolled.shape[1])])\n",
    "x_test_rolled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5fa778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html\n",
    "# Compute the one-dimensional discrete Fourier Transform for real input.\n",
    "# This function computes the one-dimensional n-point discrete Fourier Transform (DFT)\n",
    "# of a real-valued array by means of an efficient algorithm called the Fast Fourier Transform (FFT).\n",
    "x_test_fft = np.copy(x_test_rolled)\n",
    "x_test_fft = np.apply_along_axis(absfft, 1, x_test_fft)\n",
    "x_test_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_rolled.shape)\n",
    "print(x_test_fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_vars = x_test_rolled.shape[2]\n",
    "# for i in range(num_vars):\n",
    "#     mean_s = np.mean(x_test_rolled[:,:,i])\n",
    "#     sd_s = np.std(x_test_rolled[:,:,i])\n",
    "#     x_test_rolled[:,:,i] = (x_test_rolled[:,:,i]-mean_s)/sd_s\n",
    "\n",
    "# num_vars_fft = x_test_fft.shape[2]\n",
    "# for i in range(num_vars_fft):\n",
    "#     mean_s = np.mean(x_test_fft[:,:,i])\n",
    "#     sd_s = np.std(x_test_fft[:,:,i])\n",
    "#     x_test_fft[:,:,i] = (x_test_fft[:,:,i]-mean_s)/sd_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b37dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the shape x_test and x_test_fft to [observations, features, time_steps]\n",
    "x_test = x_test_rolled.transpose(0,2,1)\n",
    "x_test_fft = x_test_fft.transpose(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "print(x_test_fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the preferred device for testing to CPU\n",
    "\n",
    "# device_test = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device_test = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the testing dataset\n",
    "test_ds = TensorDataset(torch.tensor(x_test).float(), torch.tensor(x_test_fft).float())\n",
    "# Get the testing dataloader\n",
    "test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "test_preds = [] # to store prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f561054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change the model mode to evaluate and send to the device\n",
    "model.eval()\n",
    "model.to(device_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57377797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform predicting on testing date\n",
    "for test_batch in tqdm(test_dl): # Iterate through every batch of testing dataloader\n",
    "    x_raw, x_fft = [t.to(device_test) for t in test_batch] # Send the data to device\n",
    "    out = model(x_raw, x_fft) # Get the output\n",
    "    preds = F.log_softmax(out, dim=1).argmax(dim=1) # Get the final prediction\n",
    "    # decode its value for submisson compatability\n",
    "    decoded_preds = le_y.inverse_transform([preds.detach().cpu().clone().numpy()]) \n",
    "    # Save the prediction\n",
    "    test_preds.append([i for i in decoded_preds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prediction list to dataframe\n",
    "predictions = pd.DataFrame(np.array(test_preds)).reset_index()\n",
    "predictions.columns = ['id', 'ac', 'ev', 'oven', 'wash', 'dryer'] # Add column names\n",
    "predictions['id'] = predictions['id'] + 1 # For submission compatability\n",
    "predictions.set_index('id',inplace=True) # set ID column as index\n",
    "predictions.head() #have a look to check its format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many classes are predicted\n",
    "predictions[['ac', 'ev', 'oven', 'wash', 'dryer']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to .csv file for submission\n",
    "predictions.to_csv('../CNN_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88044c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82683d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f02ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44fb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45adebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70712c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.detach().cpu()\n",
    "y_batch.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a0f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_count = dict(collections.Counter(y_train_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba86781",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights = np.array([i[1] for i in sorted(cls_count.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc1e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cls_weights = torch.Tensor([cls_weights/np.sum(cls_weights)])\n",
    "cls_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights_test = torch.Tensor([cls_weights[0],1-cls_weights[0]])\n",
    "cls_weights_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d448fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53476601",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_loss = FocalLoss(gamma=2, reduction='sum', weight=cls_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86eb729",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp_loss(out, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28fa743",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f960ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fc733",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2406cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(5, (3,), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loss_weights(labels,\n",
    "                         pos_cls_weight=1.0,\n",
    "                         neg_cls_weight=1.0,\n",
    "                         loss_norm_type='norm_by_num_positives',\n",
    "                         dtype=torch.float32):\n",
    "    \"\"\"get cls_weights and reg_weights from labels.\n",
    "    \"\"\"\n",
    "    cared = labels >= 0\n",
    "    # cared: [N, num_anchors]\n",
    "    positives = labels > 0\n",
    "    negatives = labels == 0\n",
    "    negative_cls_weights = negatives.type(dtype) * neg_cls_weight\n",
    "    cls_weights = negative_cls_weights + pos_cls_weight * positives.type(dtype)\n",
    "    reg_weights = positives.type(dtype)\n",
    "    pos_normalizer = positives.sum(1, keepdim=True).type(dtype)\n",
    "    reg_weights /= torch.clamp(pos_normalizer, min=1.0)\n",
    "    cls_weights /= torch.clamp(pos_normalizer, min=1.0)\n",
    "\n",
    "    return cls_weights, reg_weights, cared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_loss_weights(torch.Tensor([y_train_transformed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0ab0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bec12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93391d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de640884",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('./np_x_train_rolled_new.npy', allow_pickle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test,dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bcde43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rolled.to_numpy(dtype=np.float64).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bd7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(x_train_rolled[:, 0:1]).astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e619a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rolled.drop('id', axis=1, inplace=True)\n",
    "x_train_rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b30b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a66455",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80eb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46119067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_rolled = pd.concat([roll_time_series(temp, column_id=\"dummy_id\", column_sort=\"id\",\n",
    "#             max_timeshift = 29, min_timeshift = 29),x_train_rolled]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01759e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"np_x_train_rolled_new.npy\",np.array(x_train_rolled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07694b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.drop(x_train.index[0:29])\n",
    "x_train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32701acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d35d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb704be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5279e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba3cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754ead9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e54b5ba8",
   "metadata": {},
   "source": [
    "## no need now - additional features calculators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = extract_features(x_train_rolled.loc[0:299999],column_id='id',column_sort='ids', \n",
    "                 default_fc_parameters=EfficientFCParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train_rolled.loc[0:299999,'id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbaa8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_selection.significance_tests import target_real_feature_real_test\n",
    "x_features_dropped = x_features.dropna(axis=1, how='any')\n",
    "keeps = {}\n",
    "for col in x_features_dropped.columns:\n",
    "    p_value = target_real_feature_real_test(x_features_dropped[col].reset_index().drop(columns=[\"level_0\",\"level_1\"],\n",
    "                                                                       axis = 1).squeeze(), \n",
    "                                     pd.Series(y_train_transformed[29:(29+60130)]))\n",
    "    if p_value < 0.01:\n",
    "        keeps[col] = p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = sorted(list(dict(sorted(keeps.items(), key=lambda item: item[1])).keys())[:255])\n",
    "x_features = x_features[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features['ids'] = x_features.index\n",
    "x_train_rolled_temp = x_train_rolled.loc[0:299999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b65326",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_temp = x_train_rolled_temp.merge(x_features, how='left', left_on=\"id\", right_on=\"ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c635d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a64f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_minimal = extract_features(x_train_rolled.loc[0:299999],column_id='id',column_sort='ids', \n",
    "                             default_fc_parameters=MinimalFCParameters())\n",
    "x_minimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1209b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(y_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfresh.feature_selection.significance_tests.target_real_feature_real_test(x_minimal['load__standard_deviation'].reset_index().drop(columns=[\"level_0\",\"level_1\"], axis = 1).squeeze(), \n",
    "                                                                               pd.Series(y_train_transformed[29:(29+60130)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d6deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_minimal.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbfe47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b987ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7114ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3162b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10000)\n",
    "data.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd689d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ca798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea2fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b75c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a0b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592a163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1643400",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = FocalLoss(alpha=0.25, gamma=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d2b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ad5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb5ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e48c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f805c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18985d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc601c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2227af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.encoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(out, torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4e7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea91e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace90034",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 33\n",
    "pks = [[4,8,1],[2,8,2],[3,8,2],[3,8,2],[3,8,2]]\n",
    "for i, layer in enumerate(pks):\n",
    "    p,k,s = layer\n",
    "    l = (l + 2*p - k)/s + 1\n",
    "    print(f\"length: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 64\n",
    "pks = [[3,8,2],[3,8,2],[3,8,2],[3,8,2],[3,8,2]]\n",
    "for i, layer in enumerate(pks):\n",
    "    p,k,s = layer\n",
    "    l = (l + 2*p - k)/s + 1\n",
    "    print(f\"length: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 30\n",
    "pks = [[4,8,2],[1,3,1],[3,8,2],[1,3,1],[5,8,2],[1,3,1],[2,8,2]]\n",
    "for i, layer in enumerate(pks):\n",
    "    p,k,s = layer\n",
    "#     w = (w + 2*p - k)/s + 1\n",
    "#     h = (h + 2*p - k)/s + 1\n",
    "    l = (l + 2*p - k)/s + 1\n",
    "    print(f\"length: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50364552",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 16\n",
    "pks = [[3,8,2],[1,3,1],[5,8,2],[1,3,1],[4,8,2],[1,3,1],[3,8,2]]\n",
    "for i, layer in enumerate(pks):\n",
    "    p,k,s = layer\n",
    "#     w = (w + 2*p - k)/s + 1\n",
    "#     h = (h + 2*p - k)/s + 1\n",
    "    l = (l + 2*p - k)/s + 1\n",
    "    print(f\"length: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "        #PKS [[4,8,2],[3,8,2],[5,8,2],[2,8,2]]\n",
    "        self.raw = nn.Sequential( #kernel, stride, pad\n",
    "            SepConv1d(raw_ni,  32, 8, 2, 4, drop=drop),\n",
    "            SepConv1d(    32,  64, 8, 2, 3, drop=drop),\n",
    "            SepConv1d(    64, 128, 8, 2, 5, drop=drop),\n",
    "            SepConv1d(   128, 256, 8, 2, 2, drop=drop),\n",
    "            Flatten(),\n",
    "#             PrintSize(),\n",
    "            nn.Dropout(drop), nn.Linear(512, 256), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(drop), nn.Linear( 256, 64), nn.ReLU(inplace=True))\n",
    "        #PKS [3,8,2],[5,8,2],[4,8,2],[5,8,2],[3,8,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a5073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f15304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40293e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b7892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5008f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0aecf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b677f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42e720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd225f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa319c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997409f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_hourly = x_train.loc[:,:].groupby([\"dayofweek\",'hourofday']).count()\n",
    "count_hourly.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b26040",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c08053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.sort_values(by=[\"dayofweek\",'hourofday']).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.sort_values(by=[\"dayofweek\",'hourofday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a06ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[(x_train.dayofweek == \"Fri\") & (x_train.hourofday == 11)].first_valid_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1770a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_hourly_obs = min(count_hourly['var'].values) \n",
    "drop_idx = []\n",
    "for _, row in count_hourly.iterrows():\n",
    "    drop_idx = [*drop_idx,*x_train[(x_train.dayofweek == row[0]) & (x_train.hourofday == row[1])].index.values[0:row[2] - min_hourly_obs]]\n",
    "drop_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fcbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(index = drop_idx, inplace = True)\n",
    "x_train.loc[:,:].groupby([\"dayofweek\",'hourofday']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [123]\n",
    "b = list(range(1,4))\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hourly_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e600f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfaa1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43c76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4cbcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb441890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a13733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "   \"id\": [1, 1, 1, 1, 2, 2],\n",
    "   \"time\": [1, 2, 3, 4, 8, 9],\n",
    "   \"x\": [1, 2, 3, 4, 10, 11],\n",
    "   \"y\": [5, 6, 7, 8, 12, 13],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bace96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b39985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "df_rolled = roll_time_series(x_train[['load','dummy_id','id']], column_id=\"dummy_id\", column_sort=\"id\",\n",
    "                            max_timeshift = 29, min_timeshift = 29)\n",
    "df_rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca6b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dcbae1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "df_rolled = roll_time_series(df, column_id=\"id\", column_sort=\"time\", max_timeshift=2, min_timeshift=2)\n",
    "df_rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf94e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_features\n",
    "df_features = extract_features(df_rolled, column_id=\"id\", column_sort=\"time\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7acadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.loc[0:500, ['load','dummy_id','id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9936445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
