{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LcAqTEOMjYMn"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install pandas_datareader\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0HH8gQW8jYMv"
   },
   "outputs": [],
   "source": [
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_rows', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCjOj0oNjYMy"
   },
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('https://raw.githubusercontent.com/minhthai1995/FIT5149/main/train_data_withlabels.csv')\n",
    "dataset_test = pd.read_csv('https://raw.githubusercontent.com/minhthai1995/FIT5149/main/test_data_nolabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DXlKjIpRjYMz",
    "outputId": "7e0bf74e-7e1e-4a56-b78f-9361b93820e6"
   },
   "outputs": [],
   "source": [
    "dataset_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Vmso9W__jYNC",
    "outputId": "34cd1a83-0b7a-41bb-fb16-ae968ccd938d"
   },
   "outputs": [],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le_iXBFKjYNF"
   },
   "outputs": [],
   "source": [
    "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIkExfOGjYNR",
    "outputId": "80599878-43de-4b93-b956-9cf39c5ee542"
   },
   "outputs": [],
   "source": [
    "dataset_train.shape, dataset_test.shape, dataset_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Qyw513xhjYNT",
    "outputId": "960fc6bb-1583-4653-eb74-b0cbc673cb68"
   },
   "outputs": [],
   "source": [
    "dataset_total.reset_index(inplace=True, drop=True) \n",
    "dataset_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f7bEuspjYNV"
   },
   "outputs": [],
   "source": [
    "scale_list = ['absdif', 'dif', 'entropy', 'hurst', 'load', 'max', 'nonlinear', 'var']\n",
    "scaler = preprocessing.MinMaxScaler().fit(dataset_total[scale_list])\n",
    "dataset_total[scale_list] = scaler.transform(dataset_total[scale_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYjX3f4ljYNa"
   },
   "outputs": [],
   "source": [
    "labelEncoderDay = preprocessing.LabelEncoder().fit(dataset_total['dayofweek'])\n",
    "labelEncoderHour = preprocessing.LabelEncoder().fit(dataset_total['hourofday'])\n",
    "dataset_total['dayofweek'] = labelEncoderDay.transform(dataset_total['dayofweek'])\n",
    "dataset_total['hourofday'] = labelEncoderHour.transform(dataset_total['hourofday'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FptlHXIxFrV"
   },
   "outputs": [],
   "source": [
    "label_list = ['ac', 'ev', 'oven', 'wash', 'dryer']\n",
    "dataset_total[label_list] = dataset_total[label_list].fillna(0.0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "1Cj3QPEbjYNd",
    "outputId": "0d3fae26-4950-4e18-d346-c81a386cd0a9"
   },
   "outputs": [],
   "source": [
    "dataset_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A00sup4kjYNf"
   },
   "outputs": [],
   "source": [
    "features_set = dataset_total.loc[:,dataset_total.columns.difference(['ac', 'ev', 'oven', 'wash', 'dryer' ])]\n",
    "features_set.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "# labels_set = dataset_total[['ac', 'ev', 'oven', 'wash', 'dryer']]\n",
    "labels_set = dataset_total[['wash']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "lksBdOQgjYNk",
    "outputId": "65f74508-8991-4476-921a-1be2e786ee34"
   },
   "outputs": [],
   "source": [
    "features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9M5iBLCq36ns"
   },
   "outputs": [],
   "source": [
    "#Take knowledge from the back 10 mins window, 1 mins each\n",
    "for x in np.arange(1, 10, 1):\n",
    "    features_set['load_before_' + str(x)] = features_set['load'].shift(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2_nMfYc36wU"
   },
   "outputs": [],
   "source": [
    "features_set = features_set.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "xOwLlkvA36zO",
    "outputId": "30fb1217-9b1b-4f54-fc00-644ef469cbc1"
   },
   "outputs": [],
   "source": [
    "features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch_ZJlq4362o"
   },
   "outputs": [],
   "source": [
    "labels_set = labels_set.loc[9:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJbpOzmF364p"
   },
   "outputs": [],
   "source": [
    "labels_set.reset_index(inplace=True, drop=True) \n",
    "features_set.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQuIml035jlf"
   },
   "outputs": [],
   "source": [
    "X_train = features_set.loc[0:(dataset_train.shape[0] - 10),]\n",
    "y_train = labels_set.loc[0:(dataset_train.shape[0] - 10), ]\n",
    "\n",
    "#X_valid = labels_set \n",
    "#y_valid = \n",
    "X_test = features_set.loc[(dataset_train.shape[0] - 10):,]\n",
    "y_test = [] \n",
    "\n",
    "# I want to use a T-days window of input data for predicting target_class\n",
    "# It means I need to prepend (T-1) last train records to the 1st test window\n",
    "T = 30  # my choice of the timesteps window\n",
    "\n",
    "prepend_features = X_train.iloc[-(T-1):]\n",
    "X_test = pd.concat([prepend_features, X_test], axis=0)\n",
    "\n",
    "# Split the training into train and valid set\n",
    "X_train_final, X_valid, y_train_final, y_valid = train_test_split(X_train, y_train, test_size = 0.25, random_state = 42, shuffle=False)\n",
    "\n",
    "prepend_features_valid = X_train_final.loc[-(T-1):]\n",
    "X_valid = pd.concat([prepend_features_valid, X_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qUGcrFS5joa",
    "outputId": "014f9a19-f319-49ab-871d-18e346c39757"
   },
   "outputs": [],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkeKqEmLjYNp"
   },
   "outputs": [],
   "source": [
    "# X_train = features_set.iloc[0:dataset_train.shape[0],]\n",
    "# y_train = labels_set.iloc[0:dataset_train.shape[0], ]\n",
    "\n",
    "# #X_valid = labels_set \n",
    "# #y_valid = \n",
    "# X_test = features_set.iloc[dataset_train.shape[0]:,]\n",
    "# y_test = [] \n",
    "\n",
    "# # I want to use a T-days window of input data for predicting target_class\n",
    "# # It means I need to prepend (T-1) last train records to the 1st test window\n",
    "# T = 30  # my choice of the timesteps window\n",
    "\n",
    "# prepend_features = X_train.iloc[-(T-1):]\n",
    "# X_test = pd.concat([prepend_features, X_test], axis=0)\n",
    "\n",
    "# # Split the training into train and valid set\n",
    "# X_train_final, X_valid, y_train_final, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42,shuffle=False)\n",
    "\n",
    "# prepend_features_valid = X_train_final.iloc[-(T-1):]\n",
    "# X_valid = pd.concat([prepend_features_valid, X_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IZmCj0SQVLc",
    "outputId": "72783643-fbff-4e0b-e702-c36118c9f9cd"
   },
   "outputs": [],
   "source": [
    "y_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIOiZ55mQVjv",
    "outputId": "0ae37bd6-0f03-4e4a-f382-87d8c184f5df"
   },
   "outputs": [],
   "source": [
    "y_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kf7T8uX0jYOD"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "for i in range(X_train_final.shape[0] - (T-1)):\n",
    "    X_train.append(X_train_final.iloc[i:i+T].values)\n",
    "    y_train.append(y_train_final.iloc[i + (T-1)].values)\n",
    "# X_train, y_train = np.array(X_train), np.array(y_train).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsrcjQ17jYOH"
   },
   "outputs": [],
   "source": [
    "X_train_LSTM, y_train_LSTM = [] , []\n",
    "X_train_LSTM = np.array(X_train)\n",
    "y_train_LSTM = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QcC_kgXjYOI",
    "outputId": "b5e3d166-7118-4d31-9760-a4854f4f3300"
   },
   "outputs": [],
   "source": [
    "X_train_LSTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaVg7_wEjYOh",
    "outputId": "d9c6ab62-9219-420a-ce79-f0d787973dc4"
   },
   "outputs": [],
   "source": [
    "y_train_LSTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhnaCRjjjYOi"
   },
   "outputs": [],
   "source": [
    "X_valid_LSTM, y_valid_LSTM = [], []\n",
    "for i in range(y_valid.shape[0]):\n",
    "    X_valid_LSTM.append(X_valid.iloc[i:i+T].values)\n",
    "    y_valid_LSTM.append(y_valid.iloc[i])\n",
    "X_valid_LSTM, y_valid_LSTM = np.array(X_valid_LSTM), np.array(y_valid_LSTM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKO8JtX2jYOj",
    "outputId": "f147f345-fec8-4888-84a9-35c4f8c8e3e0"
   },
   "outputs": [],
   "source": [
    "print(X_valid_LSTM.shape)\n",
    "print(y_valid_LSTM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNAonp3VjYOk"
   },
   "outputs": [],
   "source": [
    "X_test_LSTM = []\n",
    "for i in range(X_test.shape[0] - (T-1)):\n",
    "    X_test_LSTM.append(X_test.iloc[i:i+T].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fsy0H6ZUjYOl",
    "outputId": "24c3d638-1f73-4564-e07e-fa207125edec",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(X_test_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPYwX-1tjYOm"
   },
   "outputs": [],
   "source": [
    "X_test_LSTM = np.array(X_test_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_x9_brsqjYOn",
    "outputId": "0ff03ea2-9ab0-4ee8-879a-4cb74946aa58"
   },
   "outputs": [],
   "source": [
    "X_test_LSTM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ske0A4hijYOn"
   },
   "source": [
    "LSTM Model - Batch Training and Predictiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTr_fBlijYOo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMiuKfzIjYOp"
   },
   "outputs": [],
   "source": [
    "# Import Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xp69XP3EjYOq",
    "outputId": "9400bb43-e3cc-4acd-e9a9-67862b573f84"
   },
   "outputs": [],
   "source": [
    "# Let's make a list of CONSTANTS for modelling:\n",
    "LAYERS = [8, 8, 8, 1]                # number of units in hidden and output layers\n",
    "M_TRAIN = X_train_LSTM.shape[0]           # number of training examples (2D)\n",
    "M_TEST = X_test_LSTM.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = X_train_LSTM.shape[2]                 # number of features\n",
    "BATCH = M_TRAIN                          # batch size\n",
    "EPOCH = 200                           # number of epochs\n",
    "LR = 5e-2                            # learning rate of the gradient descent\n",
    "LAMBD = 3e-2                         # lambda in L2 regularizaion\n",
    "DP = 0.0                             # dropout rate\n",
    "RDP = 0.0                            # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6-I8kgl8Fg0"
   },
   "outputs": [],
   "source": [
    "y_1d = y_train_LSTM.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orn9oL1C00cB",
    "outputId": "931f963c-3fe9-4c07-ff77-515860739139"
   },
   "outputs": [],
   "source": [
    "pos = np.sum(y_1d == 1)\n",
    "neg = np.sum(y_1d == 0)\n",
    "print(pos/neg)\n",
    "total = pos + neg\n",
    "\n",
    "\n",
    "initial_bias = np.log([pos/neg])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27ExT_Se1Exd"
   },
   "outputs": [],
   "source": [
    "output_bias = tf.keras.initializers.Constant(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxgW2-wPjYOs"
   },
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(T, N), units=LAYERS[0], return_sequences=True\n",
    "              #  activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "              #  kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              #  dropout=DP, recurrent_dropout=RDP,\n",
    "              #  return_sequences=True, return_state=False,\n",
    "              #  stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=LAYERS[1], return_sequences=True\n",
    "              #  activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "              #  kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              #  dropout=DP, recurrent_dropout=RDP,\n",
    "              #  return_sequences=True, return_state=False,\n",
    "              #  stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=LAYERS[2],\n",
    "              #  activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "              #  kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "              #  dropout=DP, recurrent_dropout=RDP,\n",
    "              #  return_sequences=False, return_state=False,\n",
    "              #  stateful=False, unroll=False\n",
    "              ))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Dense(units=LAYERS[3], activation='sigmoid', bias_initializer=output_bias))\n",
    "\n",
    "model.add(Dense(units=LAYERS[3], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZydxoiwYEM0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkqeE7KV0gmI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEcUVjIDXl_9",
    "outputId": "bb4369ec-8f44-4681-fad0-70c4dddc64d8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy', f1],\n",
    "              optimizer= 'adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9-pR52ly6bW"
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# from math import log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7Nd2VhEjYOu",
    "outputId": "a14d35f9-c56d-488a-ae48-848e7871779a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Define a learning rate decay method:\n",
    "# lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "#                              patience=1, verbose=1, \n",
    "#                              factor=0.5, min_lr=1e-8)\n",
    "# # Define Early Stopping:\n",
    "# early_stop = EarlyStopping(monitor='val_f1', min_delta=0.01, \n",
    "#                            patience=50, verbose=1, mode='auto',\n",
    "#                            baseline=0, restore_best_weights=True)\n",
    "check_point = ModelCheckpoint('model.hdf5', monitor=\"val_f1\", mode=\"max\",\n",
    "                              verbose=True, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=100,verbose=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "classifier = model.fit(X_train_LSTM, y_train_LSTM,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0.0,\n",
    "                    validation_data=(X_valid_LSTM, y_valid_LSTM),\n",
    "                    shuffle=False,\n",
    "                    callbacks=[early_stop, check_point])\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a83KwP65jYOx",
    "outputId": "0e67c417-ba68-48c4-afa0-05b3bc9e9fc3"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model:\n",
    "train_loss, train_acc = model.evaluate(X_train_LSTM, y_train_LSTM,\n",
    "                                       batch_size=M_TRAIN, verbose=0)\n",
    "# test_loss, test_acc = model.evaluate(X_test_LSTM, y_test_LSTM,\n",
    "#                                      batch_size=M_TEST, verbose=0)\n",
    "# print('-'*65)\n",
    "print(f'train accuracy = {round(train_acc * 100, 4)}%')\n",
    "# print(f'test accuracy = {round(test_acc * 100, 4)}%')\n",
    "# print(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovyuRgyguzVJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TC_H8xEjYOy",
    "outputId": "20e3534f-e5ff-4c60-9af7-954541ec86b0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat = model.predict_classes(X_train_LSTM, batch_size = M_TRAIN, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6yhQMJqjYO1",
    "outputId": "b6d3d236-cc73-4a8f-bd7b-35719ea30673"
   },
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTobGwtmv_oy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2ZD8hL0vKD0",
    "outputId": "01a00b75-948a-4147-d756-b5ef5df6e2c3"
   },
   "outputs": [],
   "source": [
    "type(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9A5kKe8SxM3"
   },
   "outputs": [],
   "source": [
    "y_valid_1d = y_valid_LSTM.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1d = y_train_LSTM.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjJlJJWfuFBr",
    "outputId": "37a1541a-80d2-4109-8438-ef5eb3ad9d63"
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in range(0, len(yhat)):\n",
    "    if yhat[i] == y_1d[i] and yhat[i] == 1:\n",
    "        sum = sum + 1\n",
    "sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsmohXP587uM",
    "outputId": "95140a4f-bcbd-4f80-d2d8-f6e524364696"
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in yhat:\n",
    "    if i == 1:\n",
    "        sum = sum + 1\n",
    "sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in y_valid_1d:\n",
    "    if i == 1:\n",
    "        sum = sum + 1\n",
    "sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLLfXvPy86sd",
    "outputId": "9212c178-28e9-4caa-b6f7-e684db04b4e0"
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i in y_valid_1d:\n",
    "   if i == 1:\n",
    "    sum = sum + 1\n",
    "sum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMx-H-FOjYPG"
   },
   "outputs": [],
   "source": [
    "print(f1_score(y_1d, yhat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pB3ve6fujYPH",
    "outputId": "d0a8f3a6-ff79-4511-eace-e7c1cbb6145f"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_1d, yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94R_mkarvbPp"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yve0Q-DkjYPJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD_mkdecjYPS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mQoDiXYjYPT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "XdbNEUGDjYPU",
    "outputId": "79eebc18-d855-429e-adf4-5c396d286b16"
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYjGSTg-jYPU"
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdTskVNijYPV"
   },
   "outputs": [],
   "source": [
    "y_train['transformed'] = y_train.apply(lambda x: ''.join(x.astype(str)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDKsRCkujYPV"
   },
   "outputs": [],
   "source": [
    "le_y = preprocessing.LabelEncoder()\n",
    "y_train['encoded'] = le_y.fit_transform(y_train['transformed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sY4JsLpjYPX"
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7bpjREzjYPY"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(x_train['dayofweek'])\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeExXXRdjYPZ"
   },
   "outputs": [],
   "source": [
    "scale_list = ['absdif', 'dif', 'entropy', 'hurst', 'load', 'max', 'nonlinear', 'var']\n",
    "scaler = preprocessing.StandardScaler().fit(x_train[scale_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzXKfIJAjYPZ"
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ul0SZtNGjYPa"
   },
   "outputs": [],
   "source": [
    "x_train['dayofweek'] = le.transform(x_train['dayofweek'])\n",
    "x_train[scale_list] = scaler.transform(x_train[scale_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muldN2kjjYPb"
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tifRenVjYPd"
   },
   "outputs": [],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfJrlyDWjYPe"
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "x_train.plot(subplots=True,\n",
    "        layout=(6, 3),\n",
    "        figsize=(22,22),\n",
    "        fontsize=10, \n",
    "        linewidth=2,\n",
    "        sharex=False,\n",
    "        title='Visualization of the original Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FamrP5vVjYPf"
   },
   "source": [
    "The raw data contain stochastic time series. Predicting/ making classification based on stochastic variable values may force the model to learn the 'persistence' mode (i.e. yhat(t+1) = y(t)), resulting in little predictive power. Defining the model to predict (make classification from) the difference in values between the time steps rather than value itself, is a stronger test of its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-KaxalPjYPg"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.copy().pct_change(1)\n",
    "x_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZAjBpImjYPh"
   },
   "outputs": [],
   "source": [
    "x_train.fillna(method='bfill', inplace=True)\n",
    "x_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXPw4thTjYPi"
   },
   "outputs": [],
   "source": [
    "x_train.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb_1mS4njYPi"
   },
   "outputs": [],
   "source": [
    "X_train_final, X_valid, y_train_final, y_valid = train_test_split(x_train, y_train, test_size = 0.4, random_state = 42,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAMhXLdUjYPj"
   },
   "outputs": [],
   "source": [
    "# I want to use a T-mins window of input data for predicting target_class\n",
    "# It means I need to prepend (T-1) last train records to the 1st test window\n",
    "T = 30  # my choice of the timesteps window\n",
    "\n",
    "prepend_features = X_train_final.iloc[-(T-1):]\n",
    "X_valid = pd.concat([prepend_features, X_valid], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_b7aBTHjYPk"
   },
   "outputs": [],
   "source": [
    "X_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRznP8zvjYPl"
   },
   "outputs": [],
   "source": [
    "X_train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGhSwmK0jYPm"
   },
   "outputs": [],
   "source": [
    "X_train_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8VmfvsjjYPm"
   },
   "outputs": [],
   "source": [
    "X_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYqC1BlYjYP0"
   },
   "outputs": [],
   "source": [
    "X_train_final.shape, X_valid.shape, y_train_final.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERVMnvftjYP1"
   },
   "source": [
    "Input data for the Keras LSTM layer has 3 dimensions: (M, T, N), where\n",
    "\n",
    "- M - number of examples (2D: sequences of timesteps x features),\n",
    "- T - sequence length (timesteps) and\n",
    "- N - number of features (input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrZGvPY0jYP2"
   },
   "outputs": [],
   "source": [
    "X_train_final.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1TdjIp6jYP2"
   },
   "outputs": [],
   "source": [
    "# Create sequences of T timesteps\n",
    "X_train_LSTM, y_train_LSTM = [], []\n",
    "for i in range(y_train_final.shape[0] - (T-1)):\n",
    "    X_train_LSTM.append(X_train_final.iloc[i:i+T].values)\n",
    "    y_train_LSTM.append(y_train_final.iloc[i + (T-1)])\n",
    "# X_train_LSTM, y_train_LSTM = np.array(X_train_LSTM), np.array(y_train_LSTM).reshape(-1,1)\n",
    "# print(f'Train data dimensions: {X_train_LSTM.shape}, {y_train_LSTM.shape}')\n",
    "\n",
    "# X_valid, y_test = [], []\n",
    "# for i in range(test_labels.shape[0]):\n",
    "#     X_test.append(scaled_test_features.iloc[i:i+T].values)\n",
    "#     y_test.append(test_labels.iloc[i])\n",
    "# X_test, y_test = np.array(X_test), np.array(y_test).reshape(-1,1)  \n",
    "\n",
    "# print(f'Test data dimensions: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Csrbo2CjYP3"
   },
   "outputs": [],
   "source": [
    "X_train_LSTM\n",
    "X_train_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeInIx2BjYP4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wmFsha9jYP4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sniJu7zfjYP5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xKMWTn2jYP5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split train and validation data\n",
    "train_features = x_train.loc['2012-01-02':'2016-12-31']\n",
    "train_labels = df.loc['2012-01-02':'2016-12-31', 'target_class']\n",
    "\n",
    "test_features = df_transform.loc['2017-01-02':'2018-06-19']\n",
    "test_labels = df.loc['2017-01-02':'2018-06-19', 'target_class']\n",
    "\n",
    "# I want to use a T-days window of input data for predicting target_class\n",
    "# It means I need to prepend (T-1) last train records to the 1st test window\n",
    "T = 45  # my choice of the timesteps window\n",
    "\n",
    "prepend_features = train_features.iloc[-(T-1):]\n",
    "test_features = pd.concat([prepend_features, test_features], axis=0)\n",
    "\n",
    "train_features.shape, train_labels.shape, test_features.shape, test_labels.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bản sao của Bản sao của Thai Nguyen LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
